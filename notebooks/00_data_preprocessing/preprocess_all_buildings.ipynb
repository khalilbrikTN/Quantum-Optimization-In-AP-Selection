{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for All Buildings\n",
    "\n",
    "This notebook preprocesses the UJIIndoorLoc dataset for all three buildings (0, 1, 2) and saves the preprocessed data for use in subsequent experiments.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Buildings**: 0, 1, 2\n",
    "- **Preprocessing Steps**:\n",
    "  1. Load training and validation data\n",
    "  2. Filter by building ID\n",
    "  3. Normalize RSSI values\n",
    "  4. Normalize coordinates (longitude, latitude)\n",
    "  5. Save preprocessed data to pickle and Excel formats\n",
    "\n",
    "## Output\n",
    "\n",
    "For each building, the following files are generated:\n",
    "- `data/output_data/preprocessed_data/preprocessed_building_<ID>.pkl` (fast loading)\n",
    "- `data/output_data/preprocessed_data/preprocessed_building_<ID>.xlsx` (human-readable)\n",
    "- `data/system_input/system_parameters_building_<ID>.csv` (normalization parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom preprocessing modules\n",
    "from scripts.data.pre_processing import load_and_preprocess_data\n",
    "from scripts.data import pre_processing\n",
    "from scripts.data.data_loaders import save_preprocessed_data\n",
    "\n",
    "print(\"✓ Modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "data_dir = Path('data') / 'input_data'\n",
    "df_train_path = data_dir / 'TrainingData.csv'\n",
    "df_validation_path = data_dir / 'ValidationData.csv'\n",
    "\n",
    "# Building IDs to process\n",
    "building_ids = [0, 1, 2]\n",
    "\n",
    "# Floor height parameter (in meters)\n",
    "floor_height = 3.0\n",
    "\n",
    "print(f\"Training data path: {df_train_path}\")\n",
    "print(f\"Validation data path: {df_validation_path}\")\n",
    "print(f\"Buildings to process: {building_ids}\")\n",
    "print(f\"Floor height: {floor_height} meters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data to check statistics\n",
    "train_df = pd.read_csv(df_train_path)\n",
    "val_df = pd.read_csv(df_validation_path)\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total training samples: {len(train_df):,}\")\n",
    "print(f\"Total validation samples: {len(val_df):,}\")\n",
    "print(f\"\\nNumber of WAP columns: {len([col for col in train_df.columns if col.startswith('WAP')])}\")\n",
    "\n",
    "print(\"\\nSamples per building:\")\n",
    "print(\"-\"*60)\n",
    "for building_id in building_ids:\n",
    "    train_count = len(train_df[train_df['BUILDINGID'] == building_id])\n",
    "    val_count = len(val_df[val_df['BUILDINGID'] == building_id])\n",
    "    print(f\"Building {building_id}: {train_count:,} training, {val_count:,} validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess All Buildings\n",
    "\n",
    "This cell processes each building sequentially and saves the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each building\n",
    "preprocessing_summary = []\n",
    "\n",
    "for building_id in building_ids:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Processing Building {building_id}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # 1. Load and preprocess data\n",
    "        rssi_train, coords_train, rssi_val, coords_val, ap_columns = load_and_preprocess_data(\n",
    "            df_train_path, \n",
    "            df_validation_path, \n",
    "            building_id,\n",
    "            floor_height=floor_height\n",
    "        )\n",
    "        \n",
    "        # 2. Save preprocessed data\n",
    "        save_preprocessed_data(\n",
    "            rssi_train, coords_train,\n",
    "            rssi_val, coords_val,\n",
    "            ap_columns,\n",
    "            building_id=building_id\n",
    "        )\n",
    "        \n",
    "        # 3. Save system parameters (normalization values)\n",
    "        system_params_dir = Path('data') / 'system_input'\n",
    "        system_params_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        system_params = pd.DataFrame({\n",
    "            'Parameter': ['LON_MIN', 'LON_MAX', 'LAT_MIN', 'LAT_MAX', 'FLOOR_HEIGHT', 'BUILDING_ID'],\n",
    "            'Value': [\n",
    "                pre_processing.LON_MIN,\n",
    "                pre_processing.LON_MAX,\n",
    "                pre_processing.LAT_MIN,\n",
    "                pre_processing.LAT_MAX,\n",
    "                floor_height,\n",
    "                building_id\n",
    "            ],\n",
    "            'Description': [\n",
    "                'Minimum longitude value for denormalization',\n",
    "                'Maximum longitude value for denormalization',\n",
    "                'Minimum latitude value for denormalization',\n",
    "                'Maximum latitude value for denormalization',\n",
    "                'Height of each floor in meters',\n",
    "                'Building ID used for preprocessing'\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        system_params_path = system_params_dir / f'system_parameters_building_{building_id}.csv'\n",
    "        system_params.to_csv(system_params_path, index=False)\n",
    "        \n",
    "        print(f\"\\n✓ System parameters saved to: {system_params_path}\")\n",
    "        \n",
    "        # Store summary\n",
    "        preprocessing_summary.append({\n",
    "            'Building_ID': building_id,\n",
    "            'Train_Samples': len(rssi_train),\n",
    "            'Val_Samples': len(rssi_val),\n",
    "            'Num_APs': len(ap_columns),\n",
    "            'Status': 'Success'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n✓ Building {building_id} preprocessing complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error processing building {building_id}: {str(e)}\")\n",
    "        preprocessing_summary.append({\n",
    "            'Building_ID': building_id,\n",
    "            'Train_Samples': 0,\n",
    "            'Val_Samples': 0,\n",
    "            'Num_APs': 0,\n",
    "            'Status': f'Failed: {str(e)}'\n",
    "        })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Preprocessing Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(preprocessing_summary)\n",
    "\n",
    "print(\"\\nPreprocessing Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary to file\n",
    "summary_path = Path('data') / 'output_data' / 'preprocessed_data' / 'preprocessing_summary.csv'\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"\\n✓ Summary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification: Load Preprocessed Data\n",
    "\n",
    "Verify that the preprocessed data can be loaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data.data_loaders import load_preprocessed_data\n",
    "\n",
    "print(\"Verifying preprocessed data can be loaded...\\n\")\n",
    "\n",
    "for building_id in building_ids:\n",
    "    try:\n",
    "        rssi_train, coords_train, rssi_val, coords_val, ap_columns = load_preprocessed_data(\n",
    "            building_id=building_id,\n",
    "            use_pickle=True\n",
    "        )\n",
    "        print(f\"Building {building_id}: ✓ Loaded successfully\")\n",
    "        print(f\"  Training samples: {len(rssi_train)}\")\n",
    "        print(f\"  Validation samples: {len(rssi_val)}\")\n",
    "        print(f\"  Number of APs: {len(ap_columns)}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Building {building_id}: ✗ Failed to load - {str(e)}\")\n",
    "        print()\n",
    "\n",
    "print(\"\\n✓ Verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated Files\n",
    "\n",
    "The following files have been created:\n",
    "\n",
    "### Preprocessed Data (per building)\n",
    "- `data/output_data/preprocessed_data/preprocessed_building_0.pkl`\n",
    "- `data/output_data/preprocessed_data/preprocessed_building_0.xlsx`\n",
    "- `data/output_data/preprocessed_data/preprocessed_building_1.pkl`\n",
    "- `data/output_data/preprocessed_data/preprocessed_building_1.xlsx`\n",
    "- `data/output_data/preprocessed_data/preprocessed_building_2.pkl`\n",
    "- `data/output_data/preprocessed_data/preprocessed_building_2.xlsx`\n",
    "\n",
    "### System Parameters (per building)\n",
    "- `data/system_input/system_parameters_building_0.csv`\n",
    "- `data/system_input/system_parameters_building_1.csv`\n",
    "- `data/system_input/system_parameters_building_2.csv`\n",
    "\n",
    "### Summary\n",
    "- `data/output_data/preprocessed_data/preprocessing_summary.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
