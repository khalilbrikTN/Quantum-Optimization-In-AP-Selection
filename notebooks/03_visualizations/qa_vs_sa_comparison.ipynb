{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA vs SA vs All APs Comparison\n",
    "\n",
    "This notebook generates publication-ready visualizations comparing:\n",
    "- **QA**: Quantum Annealing (OpenJij SQA)\n",
    "- **SA**: Simulated Annealing  \n",
    "- **All APs**: Baseline using all 520 APs\n",
    "\n",
    "**Metrics Compared:**\n",
    "1. Floor Accuracy\n",
    "2. 3D Localization Accuracy\n",
    "3. Running Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to Python path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"✓ Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Import custom functions\n",
    "from scripts.data.data_loaders import load_preprocessed_data, load_all_precomputed_data\n",
    "from scripts.optimization.QUBO import formulate_qubo, solve_qubo_with_openjij, solve_qubo_with_SA\n",
    "from scripts.ml.ML_post_processing import train_regressor\n",
    "from scripts.evaluation.Analysis import calculate_comprehensive_metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set publication-quality plotting defaults\n",
    "SCALE_FACTOR = 1.8\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = int(12 * SCALE_FACTOR)\n",
    "plt.rcParams['axes.linewidth'] = 2 * SCALE_FACTOR\n",
    "plt.rcParams['lines.linewidth'] = 3 * SCALE_FACTOR\n",
    "plt.rcParams['xtick.major.width'] = 2 * SCALE_FACTOR\n",
    "plt.rcParams['ytick.major.width'] = 2 * SCALE_FACTOR\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print(f\"✓ Publication-quality plotting configured (DPI=300, Scale={SCALE_FACTOR}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and System Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory\n",
    "output_dir = project_root / 'data' / 'results' / 'visualizations' / 'paper'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load preprocessed data\n",
    "building_id = 1\n",
    "rssi_train, coords_train, rssi_val, coords_val, ap_columns = load_preprocessed_data(\n",
    "    building_id=building_id, use_pickle=True\n",
    ")\n",
    "\n",
    "# Initialize scaler\n",
    "scaler_coords = MinMaxScaler()\n",
    "scaler_coords.fit(coords_train)\n",
    "\n",
    "# Load importance scores and redundancy matrix\n",
    "importance_dicts, redundancy_matrix = load_all_precomputed_data()\n",
    "\n",
    "# Load system parameters\n",
    "system_params_path = project_root / 'data' / 'system_input' / 'system_parameters.csv'\n",
    "system_params_df = pd.read_csv(system_params_path)\n",
    "system_params_dict = dict(zip(system_params_df['Parameter'], system_params_df['Value']))\n",
    "\n",
    "LON_MIN = system_params_dict['LON_MIN']\n",
    "LON_MAX = system_params_dict['LON_MAX']\n",
    "LAT_MIN = system_params_dict['LAT_MIN']\n",
    "LAT_MAX = system_params_dict['LAT_MAX']\n",
    "FLOOR_HEIGHT = system_params_dict['FLOOR_HEIGHT']\n",
    "\n",
    "print(f\"✓ Loaded data: {rssi_train.shape[0]} training, {rssi_val.shape[0]} validation samples\")\n",
    "print(f\"✓ Number of APs: {len(ap_columns)}\")\n",
    "print(f\"✓ Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments: QA vs SA vs All APs\n",
    "\n",
    "We'll use the **ENTROPY** importance metric as it performed best in the pipeline experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUBO parameters\n",
    "k = 20\n",
    "alpha = 0.9\n",
    "penalty = 2.0\n",
    "\n",
    "# Use entropy importance (best performing)\n",
    "importance_method = importance_dicts['entropy']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING EXPERIMENTS: QA vs SA vs All APs\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Configuration: k={k}, alpha={alpha}, penalty={penalty}\")\n",
    "print(f\"Importance method: ENTROPY\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formulate QUBO\n",
    "print(\"\\n1. Formulating QUBO...\")\n",
    "Q, relevant_aps, offset = formulate_qubo(importance_method, redundancy_matrix, k, alpha, penalty)\n",
    "print(f\"   ✓ QUBO formulated with {len(relevant_aps)} relevant APs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Quantum Annealing (QA) with OpenJij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n2. Solving with Quantum Annealing (OpenJij SQA)...\")\n",
    "qa_indices, qa_duration = solve_qubo_with_openjij(Q)\n",
    "qa_selected_aps = [relevant_aps[i] for i in qa_indices]\n",
    "print(f\"   ✓ QA selected {len(qa_selected_aps)} APs in {qa_duration:.2f}s\")\n",
    "print(f\"   Selected APs: {', '.join(qa_selected_aps[:5])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate QA model\n",
    "print(\"   Training ML model with QA-selected APs...\")\n",
    "qa_models, qa_predictions = train_regressor(rssi_train, coords_train, rssi_val, coords_val, qa_selected_aps)\n",
    "qa_preds = qa_predictions['rf_val']\n",
    "\n",
    "print(\"   Evaluating QA model...\")\n",
    "_, _, qa_metrics = calculate_comprehensive_metrics(\n",
    "    coords_val, qa_preds, LON_MIN, LON_MAX, LAT_MIN, LAT_MAX, FLOOR_HEIGHT\n",
    ")\n",
    "print(f\"   ✓ QA Mean 3D Error: {qa_metrics['real_mean_m']:.2f}m, Floor Acc: {qa_metrics['floor_accuracy_0']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Simulated Annealing (SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n3. Solving with Simulated Annealing (SA)...\")\n",
    "sa_indices, sa_duration = solve_qubo_with_SA(Q)\n",
    "sa_selected_aps = [relevant_aps[i] for i in sa_indices]\n",
    "print(f\"   ✓ SA selected {len(sa_selected_aps)} APs in {sa_duration:.2f}s\")\n",
    "print(f\"   Selected APs: {', '.join(sa_selected_aps[:5])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate SA model\n",
    "print(\"   Training ML model with SA-selected APs...\")\n",
    "sa_models, sa_predictions = train_regressor(rssi_train, coords_train, rssi_val, coords_val, sa_selected_aps)\n",
    "sa_preds = sa_predictions['rf_val']\n",
    "\n",
    "print(\"   Evaluating SA model...\")\n",
    "_, _, sa_metrics = calculate_comprehensive_metrics(\n",
    "    coords_val, sa_preds, LON_MIN, LON_MAX, LAT_MIN, LAT_MAX, FLOOR_HEIGHT\n",
    ")\n",
    "print(f\"   ✓ SA Mean 3D Error: {sa_metrics['real_mean_m']:.2f}m, Floor Acc: {sa_metrics['floor_accuracy_0']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: All APs Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"\\n4. Training with All APs (baseline)...\")\n",
    "all_start_time = time.time()\n",
    "all_models, all_predictions = train_regressor(rssi_train, coords_train, rssi_val, coords_val, ap_columns)\n",
    "all_duration = time.time() - all_start_time\n",
    "all_preds = all_predictions['rf_val']\n",
    "\n",
    "print(\"   Evaluating All APs model...\")\n",
    "_, _, all_metrics = calculate_comprehensive_metrics(\n",
    "    coords_val, all_preds, LON_MIN, LON_MAX, LAT_MIN, LAT_MAX, FLOOR_HEIGHT\n",
    ")\n",
    "print(f\"   ✓ All APs Mean 3D Error: {all_metrics['real_mean_m']:.2f}m, Floor Acc: {all_metrics['floor_accuracy_0']:.2%}\")\n",
    "print(f\"   ✓ Training time: {all_duration:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Method': 'QA (OpenJij)',\n",
    "        'Num_APs': len(qa_selected_aps),\n",
    "        'Mean_3D_Error_m': qa_metrics['real_mean_m'],\n",
    "        'Median_3D_Error_m': qa_metrics['real_median_m'],\n",
    "        'Floor_Accuracy': qa_metrics['floor_accuracy_0'],\n",
    "        'Floor_Accuracy_±1': qa_metrics['floor_accuracy_1'],\n",
    "        'Floor_Accuracy_±2': qa_metrics['floor_accuracy_2'],\n",
    "        'Optimization_Time_s': qa_duration,\n",
    "        'All_Errors': qa_metrics['real_errors_m']\n",
    "    },\n",
    "    {\n",
    "        'Method': 'SA',\n",
    "        'Num_APs': len(sa_selected_aps),\n",
    "        'Mean_3D_Error_m': sa_metrics['real_mean_m'],\n",
    "        'Median_3D_Error_m': sa_metrics['real_median_m'],\n",
    "        'Floor_Accuracy': sa_metrics['floor_accuracy_0'],\n",
    "        'Floor_Accuracy_±1': sa_metrics['floor_accuracy_1'],\n",
    "        'Floor_Accuracy_±2': sa_metrics['floor_accuracy_2'],\n",
    "        'Optimization_Time_s': sa_duration,\n",
    "        'All_Errors': sa_metrics['real_errors_m']\n",
    "    },\n",
    "    {\n",
    "        'Method': 'All APs',\n",
    "        'Num_APs': len(ap_columns),\n",
    "        'Mean_3D_Error_m': all_metrics['real_mean_m'],\n",
    "        'Median_3D_Error_m': all_metrics['real_median_m'],\n",
    "        'Floor_Accuracy': all_metrics['floor_accuracy_0'],\n",
    "        'Floor_Accuracy_±1': all_metrics['floor_accuracy_1'],\n",
    "        'Floor_Accuracy_±2': all_metrics['floor_accuracy_2'],\n",
    "        'Optimization_Time_s': all_duration,\n",
    "        'All_Errors': all_metrics['real_errors_m']\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df[['Method', 'Num_APs', 'Mean_3D_Error_m', 'Floor_Accuracy', 'Optimization_Time_s']].to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: Floor Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot 1: Grouped Bar Chart\n",
    "methods = comparison_df['Method'].tolist()\n",
    "x_pos = np.arange(len(methods))\n",
    "width = 0.25\n",
    "\n",
    "floor_0 = comparison_df['Floor_Accuracy'].tolist()\n",
    "floor_1 = comparison_df['Floor_Accuracy_±1'].tolist()\n",
    "floor_2 = comparison_df['Floor_Accuracy_±2'].tolist()\n",
    "\n",
    "bars1 = axes[0].bar(x_pos - width, floor_0, width, label='Exact Floor', color='#2E86AB', edgecolor='black', linewidth=2)\n",
    "bars2 = axes[0].bar(x_pos, floor_1, width, label='±1 Floor', color='#A23B72', edgecolor='black', linewidth=2)\n",
    "bars3 = axes[0].bar(x_pos + width, floor_2, width, label='±2 Floors', color='#F18F01', edgecolor='black', linewidth=2)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.1%}', ha='center', va='bottom', fontweight='bold', fontsize=14)\n",
    "\n",
    "axes[0].set_xlabel('Method', fontsize=16, fontweight='bold')\n",
    "axes[0].set_ylabel('Floor Accuracy', fontsize=16, fontweight='bold')\n",
    "axes[0].set_title('Floor Accuracy: QA vs SA vs All APs', fontsize=18, fontweight='bold', pad=20)\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(methods, fontsize=14)\n",
    "axes[0].legend(fontsize=14, loc='lower right')\n",
    "axes[0].set_ylim([0, 1.1])\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: CDF of Floor Accuracy\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    errors = row['All_Errors']\n",
    "    sorted_errors = np.sort(errors)\n",
    "    cdf = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "    axes[1].plot(sorted_errors, cdf, linewidth=3, label=row['Method'], marker='o', markersize=4, markevery=10)\n",
    "\n",
    "axes[1].set_xlabel('3D Localization Error (m)', fontsize=16, fontweight='bold')\n",
    "axes[1].set_ylabel('Cumulative Probability', fontsize=16, fontweight='bold')\n",
    "axes[1].set_title('CDF of 3D Localization Error', fontsize=18, fontweight='bold', pad=20)\n",
    "axes[1].legend(fontsize=14, loc='lower right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim([0, 40])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'floor_accuracy_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Figure 1 saved: floor_accuracy_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: 3D Localization Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot 1: Bar chart for Mean and Median errors\n",
    "x_pos = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "mean_errors = comparison_df['Mean_3D_Error_m'].tolist()\n",
    "median_errors = comparison_df['Median_3D_Error_m'].tolist()\n",
    "\n",
    "bars1 = axes[0].bar(x_pos - width/2, mean_errors, width, label='Mean Error', \n",
    "                    color='#2E86AB', edgecolor='black', linewidth=2)\n",
    "bars2 = axes[0].bar(x_pos + width/2, median_errors, width, label='Median Error', \n",
    "                    color='#A23B72', edgecolor='black', linewidth=2)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.1f}m', ha='center', va='bottom', fontweight='bold', fontsize=14)\n",
    "\n",
    "axes[0].set_xlabel('Method', fontsize=16, fontweight='bold')\n",
    "axes[0].set_ylabel('3D Localization Error (m)', fontsize=16, fontweight='bold')\n",
    "axes[0].set_title('3D Localization Accuracy: QA vs SA vs All APs', fontsize=18, fontweight='bold', pad=20)\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(methods, fontsize=14)\n",
    "axes[0].legend(fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Box plot of error distributions\n",
    "error_data = [row['All_Errors'] for _, row in comparison_df.iterrows()]\n",
    "bp = axes[1].boxplot(error_data, labels=methods, patch_artist=True,\n",
    "                     boxprops=dict(facecolor='#2E86AB', alpha=0.7, linewidth=2),\n",
    "                     medianprops=dict(color='red', linewidth=3),\n",
    "                     whiskerprops=dict(linewidth=2),\n",
    "                     capprops=dict(linewidth=2),\n",
    "                     flierprops=dict(marker='o', markersize=6, alpha=0.5))\n",
    "\n",
    "axes[1].set_xlabel('Method', fontsize=16, fontweight='bold')\n",
    "axes[1].set_ylabel('3D Localization Error (m)', fontsize=16, fontweight='bold')\n",
    "axes[1].set_title('Error Distribution: QA vs SA vs All APs', fontsize=18, fontweight='bold', pad=20)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / '3d_localization_accuracy_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Figure 2 saved: 3d_localization_accuracy_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3: Running Time Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Extract running times\n",
    "times = comparison_df['Optimization_Time_s'].tolist()\n",
    "x_pos = np.arange(len(methods))\n",
    "\n",
    "# Create bar chart\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "bars = ax.bar(x_pos, times, color=colors, edgecolor='black', linewidth=2, alpha=0.8)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.2f}s', ha='center', va='bottom', fontweight='bold', fontsize=16)\n",
    "\n",
    "ax.set_xlabel('Method', fontsize=18, fontweight='bold')\n",
    "ax.set_ylabel('Running Time (seconds)', fontsize=18, fontweight='bold')\n",
    "ax.set_title('Running Time Comparison: QA vs SA vs All APs', fontsize=20, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(methods, fontsize=16)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'running_time_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Figure 3 saved: running_time_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "results_file = project_root / 'data' / 'results' / 'qa_vs_sa_comparison.xlsx'\n",
    "comparison_df_save = comparison_df.drop(columns=['All_Errors'])\n",
    "comparison_df_save.to_excel(results_file, index=False)\n",
    "print(f\"✓ Comparison results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Floor Accuracy Analysis\n",
    "The results show that quantum annealing achieves competitive floor accuracy compared to using all APs while using significantly fewer access points.\n",
    "\n",
    "### 3D Localization Accuracy Analysis  \n",
    "Both QA and SA methods achieve comparable localization accuracy to the all-APs baseline while dramatically reducing the number of required access points from 520 to 20.\n",
    "\n",
    "### Running Time Analysis\n",
    "The quantum annealing approach demonstrates computational efficiency improvements over traditional methods, particularly important for real-time deployment scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
