{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annealing Time Comparison: SA vs SQA (OpenJij Method)\n",
    "\n",
    "This notebook compares **actual annealing time** between SA and SQA using **OpenJij's official methodology**.\n",
    "\n",
    "**Based on:** https://tutorial.openjij.org/en/tutorial/005-Evaluation.html\n",
    "\n",
    "**Key Metrics:**\n",
    "1. **Execution Time**: From `response.info['execution_time']` (OpenJij's actual measurement)\n",
    "2. **Success Probability**: `p_s = n/R` (successful attempts / total runs)\n",
    "3. **Time-to-Solution (TTS)**: `TTS = τ × [ln(1 - p_R) / ln(1 - p_s)]`\n",
    "   - τ = single annealing time\n",
    "   - p_R = target probability (typically 0.99)\n",
    "   - p_s = success probability\n",
    "\n",
    "**Methods Compared:**\n",
    "- **SQA**: Simulated Quantum Annealing (OpenJij)\n",
    "- **SA**: Simulated Annealing (D-Wave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to Python path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"✓ Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "import openjij as oj\n",
    "from dwave.samplers import SimulatedAnnealingSampler\n",
    "import dimod\n",
    "\n",
    "# Import custom functions\n",
    "from scripts.data.data_loaders import load_all_precomputed_data\n",
    "from scripts.optimization.QUBO import formulate_qubo\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set publication-quality plotting defaults\n",
    "SCALE_FACTOR = 1.8\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = int(12 * SCALE_FACTOR)\n",
    "plt.rcParams['axes.linewidth'] = 2 * SCALE_FACTOR\n",
    "plt.rcParams['lines.linewidth'] = 3 * SCALE_FACTOR\n",
    "plt.rcParams['xtick.major.width'] = 2 * SCALE_FACTOR\n",
    "plt.rcParams['ytick.major.width'] = 2 * SCALE_FACTOR\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print(f\"✓ Publication-quality plotting configured (DPI=300, Scale={SCALE_FACTOR}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and QUBO Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory\n",
    "output_dir = project_root / 'data' / 'results' / 'visualizations' / 'paper'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load importance scores and redundancy matrix\n",
    "importance_dicts, redundancy_matrix = load_all_precomputed_data()\n",
    "\n",
    "# Use entropy importance (best performing)\n",
    "importance_method = importance_dicts['entropy']\n",
    "\n",
    "print(f\"✓ Data loaded successfully\")\n",
    "print(f\"✓ Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUBO parameters\n",
    "k = 20\n",
    "alpha = 0.9\n",
    "penalty = 2.0\n",
    "\n",
    "# Annealing parameters\n",
    "num_sweeps_test = [100, 500, 1000, 2000, 5000]  # Different annealing durations\n",
    "num_reads_fixed = 100  # Fixed for sweep tests\n",
    "num_sweeps_fixed = 1000  # Fixed for reads tests\n",
    "num_reads_test = [10, 50, 100, 500, 1000]  # Different sampling counts\n",
    "\n",
    "# Number of repetitions for statistical analysis\n",
    "num_repetitions = 10\n",
    "\n",
    "# Target success probability for TTS calculation\n",
    "p_target = 0.99\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ANNEALING TIME COMPARISON: SA vs SQA (OpenJij Method)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"QUBO Configuration: k={k}, alpha={alpha}, penalty={penalty}\")\n",
    "print(f\"Sweep values to test: {num_sweeps_test}\")\n",
    "print(f\"Read values to test: {num_reads_test}\")\n",
    "print(f\"Repetitions per configuration: {num_repetitions}\")\n",
    "print(f\"Target success probability (p_R): {p_target}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulate QUBO Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Formulating QUBO problem...\")\n",
    "Q, relevant_aps, offset = formulate_qubo(importance_method, redundancy_matrix, k, alpha, penalty)\n",
    "print(f\"✓ QUBO formulated with {len(relevant_aps)} relevant APs\")\n",
    "print(f\"✓ QUBO size: {len(Q)} terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tts(execution_time, success_prob, p_target=0.99):\n",
    "    \"\"\"\n",
    "    Calculate Time-to-Solution (TTS) using OpenJij's formula.\n",
    "    \n",
    "    TTS = τ × [ln(1 - p_R) / ln(1 - p_s)]\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    execution_time : float\n",
    "        Single annealing execution time (τ)\n",
    "    success_prob : float\n",
    "        Success probability (p_s)\n",
    "    p_target : float\n",
    "        Target success probability (p_R), default 0.99\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Time-to-Solution\n",
    "    \"\"\"\n",
    "    if success_prob <= 0 or success_prob >= 1:\n",
    "        return np.inf\n",
    "    \n",
    "    return execution_time * (np.log(1 - p_target) / np.log(1 - success_prob))\n",
    "\n",
    "def get_best_energy(Q):\n",
    "    \"\"\"\n",
    "    Get the best known energy for the QUBO problem (for success probability calculation).\n",
    "    Run multiple times and take the minimum.\n",
    "    \"\"\"\n",
    "    sampler = oj.SQASampler()\n",
    "    best_energy = np.inf\n",
    "    \n",
    "    for _ in range(50):  # Run 50 times to find best\n",
    "        response = sampler.sample_qubo(Q, num_reads=100, num_sweeps=5000)\n",
    "        best_energy = min(best_energy, response.first.energy)\n",
    "    \n",
    "    return best_energy\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Best Known Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finding best known energy for the QUBO problem...\")\n",
    "best_known_energy = get_best_energy(Q)\n",
    "print(f\"✓ Best known energy: {best_known_energy:.4f}\")\n",
    "print(f\"  (Using tolerance of 1% for success determination)\")\n",
    "energy_tolerance = abs(best_known_energy) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 1: SQA (OpenJij) - Varying num_sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"BENCHMARK 1: SQA - Varying num_sweeps (num_reads={num_reads_fixed})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sqa_results_sweeps = []\n",
    "\n",
    "for num_sweeps in num_sweeps_test:\n",
    "    print(f\"\\nTesting SQA with num_sweeps={num_sweeps}...\")\n",
    "    \n",
    "    execution_times = []\n",
    "    successes = 0\n",
    "    \n",
    "    for rep in range(num_repetitions):\n",
    "        sampler = oj.SQASampler()\n",
    "        response = sampler.sample_qubo(Q, num_reads=num_reads_fixed, num_sweeps=num_sweeps)\n",
    "        \n",
    "        # Get execution time from OpenJij's response\n",
    "        exec_time = response.info.get('execution_time', 0)\n",
    "        execution_times.append(exec_time)\n",
    "        \n",
    "        # Check if solution is successful (within tolerance of best energy)\n",
    "        if abs(response.first.energy - best_known_energy) <= energy_tolerance:\n",
    "            successes += 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_exec_time = np.mean(execution_times)\n",
    "    success_prob = successes / num_repetitions\n",
    "    tts = calculate_tts(avg_exec_time, success_prob, p_target)\n",
    "    \n",
    "    sqa_results_sweeps.append({\n",
    "        'num_sweeps': num_sweeps,\n",
    "        'num_reads': num_reads_fixed,\n",
    "        'avg_execution_time': avg_exec_time,\n",
    "        'std_execution_time': np.std(execution_times),\n",
    "        'success_probability': success_prob,\n",
    "        'tts': tts,\n",
    "        'successes': successes,\n",
    "        'total_runs': num_repetitions\n",
    "    })\n",
    "    \n",
    "    print(f\"  Avg execution time: {avg_exec_time:.6f}s\")\n",
    "    print(f\"  Success probability: {success_prob:.2%} ({successes}/{num_repetitions})\")\n",
    "    print(f\"  TTS: {tts:.6f}s\")\n",
    "\n",
    "df_sqa_sweeps = pd.DataFrame(sqa_results_sweeps)\n",
    "print(f\"\\n✓ SQA sweep benchmark complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 2: SA (D-Wave) - Varying num_sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"BENCHMARK 2: SA - Varying num_sweeps (num_reads={num_reads_fixed})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sa_results_sweeps = []\n",
    "\n",
    "for num_sweeps in num_sweeps_test:\n",
    "    print(f\"\\nTesting SA with num_sweeps={num_sweeps}...\")\n",
    "    \n",
    "    execution_times = []\n",
    "    successes = 0\n",
    "    \n",
    "    for rep in range(num_repetitions):\n",
    "        bqm = dimod.BinaryQuadraticModel(Q, 'BINARY')\n",
    "        sampler = SimulatedAnnealingSampler()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = sampler.sample(bqm, num_reads=num_reads_fixed, num_sweeps=num_sweeps, beta_range=(0.1, 5.0))\n",
    "        exec_time = time.time() - start_time\n",
    "        \n",
    "        execution_times.append(exec_time)\n",
    "        \n",
    "        # Check if solution is successful\n",
    "        if abs(response.first.energy - best_known_energy) <= energy_tolerance:\n",
    "            successes += 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_exec_time = np.mean(execution_times)\n",
    "    success_prob = successes / num_repetitions\n",
    "    tts = calculate_tts(avg_exec_time, success_prob, p_target)\n",
    "    \n",
    "    sa_results_sweeps.append({\n",
    "        'num_sweeps': num_sweeps,\n",
    "        'num_reads': num_reads_fixed,\n",
    "        'avg_execution_time': avg_exec_time,\n",
    "        'std_execution_time': np.std(execution_times),\n",
    "        'success_probability': success_prob,\n",
    "        'tts': tts,\n",
    "        'successes': successes,\n",
    "        'total_runs': num_repetitions\n",
    "    })\n",
    "    \n",
    "    print(f\"  Avg execution time: {avg_exec_time:.6f}s\")\n",
    "    print(f\"  Success probability: {success_prob:.2%} ({successes}/{num_repetitions})\")\n",
    "    print(f\"  TTS: {tts:.6f}s\")\n",
    "\n",
    "df_sa_sweeps = pd.DataFrame(sa_results_sweeps)\n",
    "print(f\"\\n✓ SA sweep benchmark complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: Execution Time vs num_sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot 1: Execution Time\n",
    "axes[0].errorbar(df_sqa_sweeps['num_sweeps'], df_sqa_sweeps['avg_execution_time'],\n",
    "                yerr=df_sqa_sweeps['std_execution_time'],\n",
    "                marker='o', markersize=8, linewidth=2.5, capsize=5,\n",
    "                label='SQA (OpenJij)', color='#2E86AB')\n",
    "axes[0].errorbar(df_sa_sweeps['num_sweeps'], df_sa_sweeps['avg_execution_time'],\n",
    "                yerr=df_sa_sweeps['std_execution_time'],\n",
    "                marker='s', markersize=8, linewidth=2.5, capsize=5,\n",
    "                label='SA (D-Wave)', color='#A23B72')\n",
    "\n",
    "axes[0].set_xlabel('Number of Sweeps', fontsize=18, fontweight='bold')\n",
    "axes[0].set_ylabel('Execution Time (seconds)', fontsize=18, fontweight='bold')\n",
    "axes[0].set_title(f'Execution Time vs num_sweeps (num_reads={num_reads_fixed})',\n",
    "                 fontsize=20, fontweight='bold', pad=20)\n",
    "axes[0].legend(fontsize=16, loc='upper left')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Plot 2: Success Probability\n",
    "axes[1].plot(df_sqa_sweeps['num_sweeps'], df_sqa_sweeps['success_probability'] * 100,\n",
    "            marker='o', markersize=8, linewidth=2.5,\n",
    "            label='SQA (OpenJij)', color='#2E86AB')\n",
    "axes[1].plot(df_sa_sweeps['num_sweeps'], df_sa_sweeps['success_probability'] * 100,\n",
    "            marker='s', markersize=8, linewidth=2.5,\n",
    "            label='SA (D-Wave)', color='#A23B72')\n",
    "\n",
    "axes[1].set_xlabel('Number of Sweeps', fontsize=18, fontweight='bold')\n",
    "axes[1].set_ylabel('Success Probability (%)', fontsize=18, fontweight='bold')\n",
    "axes[1].set_title('Success Probability vs num_sweeps',\n",
    "                 fontsize=20, fontweight='bold', pad=20)\n",
    "axes[1].legend(fontsize=16, loc='lower right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'annealing_time_execution_success.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Figure 1 saved: annealing_time_execution_success.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: Time-to-Solution (TTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Filter out infinite TTS values\n",
    "sqa_finite = df_sqa_sweeps[df_sqa_sweeps['tts'] != np.inf]\n",
    "sa_finite = df_sa_sweeps[df_sa_sweeps['tts'] != np.inf]\n",
    "\n",
    "ax.plot(sqa_finite['num_sweeps'], sqa_finite['tts'],\n",
    "       marker='o', markersize=10, linewidth=3,\n",
    "       label='SQA (OpenJij)', color='#2E86AB')\n",
    "ax.plot(sa_finite['num_sweeps'], sa_finite['tts'],\n",
    "       marker='s', markersize=10, linewidth=3,\n",
    "       label='SA (D-Wave)', color='#A23B72')\n",
    "\n",
    "ax.set_xlabel('Number of Sweeps', fontsize=18, fontweight='bold')\n",
    "ax.set_ylabel(f'Time-to-Solution (seconds, p={p_target})', fontsize=18, fontweight='bold')\n",
    "ax.set_title('Time-to-Solution Comparison: SQA vs SA',\n",
    "            fontsize=20, fontweight='bold', pad=20)\n",
    "ax.legend(fontsize=16, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'annealing_time_tts_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Figure 2 saved: annealing_time_tts_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANNEALING TIME COMPARISON SUMMARY (OpenJij Method)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nBest Known Energy: {best_known_energy:.4f}\")\n",
    "print(f\"Energy Tolerance: ±{energy_tolerance:.4f}\")\n",
    "\n",
    "print(\"\\n1. SQA (OpenJij) Results:\")\n",
    "print(\"-\" * 80)\n",
    "print(df_sqa_sweeps[['num_sweeps', 'avg_execution_time', 'success_probability', 'tts']].to_string(index=False))\n",
    "\n",
    "print(\"\\n2. SA (D-Wave) Results:\")\n",
    "print(\"-\" * 80)\n",
    "print(df_sa_sweeps[['num_sweeps', 'avg_execution_time', 'success_probability', 'tts']].to_string(index=False))\n",
    "\n",
    "print(\"\\n3. Key Findings:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"- Execution times measured using OpenJij's response.info['execution_time']\")\n",
    "print(f\"- Success probability based on {energy_tolerance:.4f} tolerance from optimal\")\n",
    "print(f\"- TTS calculated with target probability p_R = {p_target}\")\n",
    "print(f\"- Higher num_sweeps generally increases success probability\")\n",
    "print(f\"- TTS provides fair comparison accounting for solution quality\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "results_file = project_root / 'data' / 'results' / 'annealing_time_openjij_method.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(results_file) as writer:\n",
    "    df_sqa_sweeps.to_excel(writer, sheet_name='SQA_Sweeps', index=False)\n",
    "    df_sa_sweeps.to_excel(writer, sheet_name='SA_Sweeps', index=False)\n",
    "    \n",
    "    # Summary sheet\n",
    "    summary_data = pd.DataFrame([{\n",
    "        'Best_Known_Energy': best_known_energy,\n",
    "        'Energy_Tolerance': energy_tolerance,\n",
    "        'Target_Probability': p_target,\n",
    "        'Num_Repetitions': num_repetitions\n",
    "    }])\n",
    "    summary_data.to_excel(writer, sheet_name='Config', index=False)\n",
    "\n",
    "print(f\"✓ Results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This analysis uses **OpenJij's official methodology** for annealing time evaluation:\n",
    "\n",
    "### Key Metrics:\n",
    "1. **Execution Time**: Direct measurement from `response.info['execution_time']`\n",
    "2. **Success Probability**: Fraction of runs achieving near-optimal solution\n",
    "3. **Time-to-Solution (TTS)**: Accounts for both speed and quality\n",
    "\n",
    "### Advantages of TTS:\n",
    "- Provides **fair comparison** between methods with different success rates\n",
    "- Answers: \"How long to get optimal solution with 99% confidence?\"\n",
    "- Combines execution time and solution quality into single metric\n",
    "\n",
    "### Insights:\n",
    "- SQA and SA show different trade-offs between speed and success rate\n",
    "- Optimal num_sweeps minimizes TTS (not necessarily execution time)\n",
    "- Framework overhead visible in execution time measurements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
