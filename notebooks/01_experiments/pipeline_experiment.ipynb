{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# System Pipeline: Load Preprocessed Data, Importance Scores, and Redundancy\n",
    "\n",
    "This notebook demonstrates the complete pipeline for loading:\n",
    "1. Preprocessed data for Building 1\n",
    "2. Importance metrics from saved scores\n",
    "3. Redundancy data\n",
    "\n",
    "This is a streamlined version that loads pre-computed data without re-running expensive computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f247a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Python: C:\\Users\\Mohamed Khalil\\Desktop\\Quantum-Optimization-In-AP-Selection\\venv\\Scripts\\python.exe\n",
      "\n",
      "✓ Packages installed!\n",
      "Now restart your kernel: Kernel → Restart Kernel\n",
      "Then re-run all cells\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(f\"Current Python: {sys.executable}\")\n",
    "\n",
    "# Install packages in the current notebook environment\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openjij\", \"dwave-ocean-sdk\", \"-q\"])\n",
    "\n",
    "print(\"\\n✓ Packages installed!\")\n",
    "print(\"Now restart your kernel: Kernel → Restart Kernel\")\n",
    "print(\"Then re-run all cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "j0f9mfzfv1k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Added project root to Python path: c:\\Users\\Mohamed Khalil\\Desktop\\Quantum-Optimization-In-AP-Selection\n"
     ]
    }
   ],
   "source": [
    "# Add project root to Python path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the project root (2 levels up from this notebook)\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"✓ Added project root to Python path: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openjij'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     load_preprocessed_data,\n\u001b[32m     10\u001b[39m     load_all_precomputed_data,\n\u001b[32m     11\u001b[39m     load_importance_dict_from_csv,\n\u001b[32m     12\u001b[39m     load_redundancy_matrix_from_csv\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Import QUBO optimization functions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mQUBO\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     formulate_qubo,\n\u001b[32m     18\u001b[39m     solve_qubo_with_openjij,\n\u001b[32m     19\u001b[39m     solve_qubo_with_SA\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Import ML training functions\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mML_post_processing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_regressor\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mohamed Khalil\\Desktop\\Quantum-Optimization-In-AP-Selection\\scripts\\optimization\\QUBO.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenjij\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moj\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdimod\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openjij'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "# Import custom data loading functions\n",
    "from scripts.data.data_loaders import (\n",
    "    load_preprocessed_data,\n",
    "    load_all_precomputed_data,\n",
    "    load_importance_dict_from_csv,\n",
    "    load_redundancy_matrix_from_csv\n",
    ")\n",
    "\n",
    "# Import QUBO optimization functions\n",
    "from scripts.optimization.QUBO import (\n",
    "    formulate_qubo,\n",
    "    solve_qubo_with_openjij,\n",
    "    solve_qubo_with_SA\n",
    ")\n",
    "\n",
    "# Import ML training functions\n",
    "from scripts.ml.ML_post_processing import train_regressor\n",
    "\n",
    "# Import evaluation functions\n",
    "from scripts.evaluation.Analysis import calculate_comprehensive_metrics\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1_header",
   "metadata": {},
   "source": [
    "## Step 1: Load Preprocessed Data for Building 1\n",
    "\n",
    "This loads the preprocessed RSSI data, coordinates, and AP columns from saved files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_preprocessed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify building ID\n",
    "building_id = 1\n",
    "\n",
    "# Load preprocessed data (uses pickle for fast loading)\n",
    "rssi_train, coords_train, rssi_val, coords_val, ap_columns = load_preprocessed_data(\n",
    "    building_id=building_id,\n",
    "    use_pickle=True  # True = fast (pickle), False = slower (Excel)\n",
    ")\n",
    "\n",
    "# Initialize and fit the coordinate scaler\n",
    "scaler_coords = MinMaxScaler()\n",
    "scaler_coords.fit(coords_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSED DATA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Building ID: {building_id}\")\n",
    "print(f\"Training samples: {rssi_train.shape[0]}\")\n",
    "print(f\"Validation samples: {rssi_val.shape[0]}\")\n",
    "print(f\"Number of APs: {len(ap_columns)}\")\n",
    "print(f\"\\nRSSI Training shape: {rssi_train.shape}\")\n",
    "print(f\"Coordinates Training shape: {coords_train.shape}\")\n",
    "print(f\"RSSI Validation shape: {rssi_val.shape}\")\n",
    "print(f\"Coordinates Validation shape: {coords_val.shape}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2_header",
   "metadata": {},
   "source": [
    "## Step 2: Load Importance Scores\n",
    "\n",
    "This loads all pre-computed importance metrics from saved CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all importance dictionaries at once\n",
    "importance_dicts, _ = load_all_precomputed_data()\n",
    "\n",
    "# Access individual importance methods\n",
    "importance_entropy = importance_dicts['entropy']\n",
    "importance_average = importance_dicts['average']\n",
    "importance_median = importance_dicts['median']\n",
    "importance_max = importance_dicts['max']\n",
    "importance_variance = importance_dicts['variance']\n",
    "importance_mutual_info = importance_dicts['mutual_info']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPORTANCE SCORES SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Entropy importance: {len(importance_entropy)} APs\")\n",
    "print(f\"Average importance: {len(importance_average)} APs\")\n",
    "print(f\"Median importance: {len(importance_median)} APs\")\n",
    "print(f\"Max importance: {len(importance_max)} APs\")\n",
    "print(f\"Variance importance: {len(importance_variance)} APs\")\n",
    "print(f\"Mutual Info importance: {len(importance_mutual_info)} APs\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show top 5 APs for each method\n",
    "print(\"\\nTop 5 APs by Entropy Importance:\")\n",
    "top_entropy = sorted(importance_entropy.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for ap, score in top_entropy:\n",
    "    print(f\"  {ap}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 APs by Mutual Information:\")\n",
    "top_mi = sorted(importance_mutual_info.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for ap, score in top_mi:\n",
    "    print(f\"  {ap}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3_header",
   "metadata": {},
   "source": [
    "## Step 3: Load Redundancy Matrix\n",
    "\n",
    "This loads the pre-computed redundancy matrix from saved files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_redundancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load redundancy matrix (second return value from load_all_precomputed_data)\n",
    "_, redundancy_matrix = load_all_precomputed_data()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REDUNDANCY MATRIX SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Matrix shape: {redundancy_matrix.shape}\")\n",
    "print(f\"Matrix type: {type(redundancy_matrix)}\")\n",
    "print(f\"\\nRedundancy statistics:\")\n",
    "print(f\"  Mean redundancy: {redundancy_matrix.values.mean():.4f}\")\n",
    "print(f\"  Min redundancy: {redundancy_matrix.values.min():.4f}\")\n",
    "print(f\"  Max redundancy: {redundancy_matrix.values.max():.4f}\")\n",
    "print(f\"  Median redundancy: {np.median(redundancy_matrix.values):.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show a sample of the redundancy matrix\n",
    "print(\"\\nSample of redundancy matrix (first 5x5):\")\n",
    "print(redundancy_matrix.iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verification_header",
   "metadata": {},
   "source": [
    "## Step 4: Verification and Summary\n",
    "\n",
    "Verify that all data has been loaded correctly and is ready for use in the optimization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all components are loaded\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE DATA VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checks = [\n",
    "    (\"Preprocessed training data\", rssi_train is not None and len(rssi_train) > 0),\n",
    "    (\"Preprocessed validation data\", rssi_val is not None and len(rssi_val) > 0),\n",
    "    (\"Training coordinates\", coords_train is not None and len(coords_train) > 0),\n",
    "    (\"Validation coordinates\", coords_val is not None and len(coords_val) > 0),\n",
    "    (\"AP columns\", ap_columns is not None and len(ap_columns) > 0),\n",
    "    (\"Entropy importance\", len(importance_entropy) > 0),\n",
    "    (\"Average importance\", len(importance_average) > 0),\n",
    "    (\"Median importance\", len(importance_median) > 0),\n",
    "    (\"Max importance\", len(importance_max) > 0),\n",
    "    (\"Variance importance\", len(importance_variance) > 0),\n",
    "    (\"Mutual info importance\", len(importance_mutual_info) > 0),\n",
    "    (\"Redundancy matrix\", redundancy_matrix is not None and redundancy_matrix.shape[0] > 0),\n",
    "]\n",
    "\n",
    "all_passed = True\n",
    "for check_name, result in checks:\n",
    "    status = \"✓\" if result else \"✗\"\n",
    "    print(f\"{status} {check_name}\")\n",
    "    if not result:\n",
    "        all_passed = False\n",
    "\n",
    "print(\"=\"*60)\n",
    "if all_passed:\n",
    "    print(\"\\n✓ ALL DATA LOADED SUCCESSFULLY!\")\n",
    "    print(\"\\nThe pipeline is ready. You can now:\")\n",
    "    print(\"  1. Run QUBO optimization with different importance metrics\")\n",
    "    print(\"  2. Train ML models on selected AP subsets\")\n",
    "    print(\"  3. Evaluate positioning accuracy\")\n",
    "else:\n",
    "    print(\"\\n✗ SOME DATA FAILED TO LOAD. Please check the above errors.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "## Step 5: Load System Parameters\n",
    "\n",
    "Load normalization parameters and configure QUBO settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placeholder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load system parameters from CSV\n",
    "system_params_path = Path('../../data') / 'system_input' / 'system_parameters.csv'\n",
    "system_params_df = pd.read_csv(system_params_path)\n",
    "\n",
    "# Convert to dictionary for easy access\n",
    "system_params_dict = dict(zip(system_params_df['Parameter'], system_params_df['Value']))\n",
    "\n",
    "# Extract parameters\n",
    "LON_MIN = system_params_dict['LON_MIN']\n",
    "LON_MAX = system_params_dict['LON_MAX']\n",
    "LAT_MIN = system_params_dict['LAT_MIN']\n",
    "LAT_MAX = system_params_dict['LAT_MAX']\n",
    "FLOOR_HEIGHT = system_params_dict['FLOOR_HEIGHT']\n",
    "\n",
    "# QUBO parameters\n",
    "k = 20  # Number of APs to select\n",
    "alpha = 0.9  # Importance vs redundancy trade-off (higher = more importance weight)\n",
    "penalty = 2.0  # Penalty for violating the k constraint\n",
    "\n",
    "print(\"✓ System parameters loaded from CSV:\")\n",
    "print(f\"  LON_MIN: {LON_MIN}\")\n",
    "print(f\"  LON_MAX: {LON_MAX}\")\n",
    "print(f\"  LAT_MIN: {LAT_MIN}\")\n",
    "print(f\"  LAT_MAX: {LAT_MAX}\")\n",
    "print(f\"  FLOOR_HEIGHT: {FLOOR_HEIGHT}\")\n",
    "print(f\"\\nQUBO parameters:\")\n",
    "print(f\"  k (num APs to select): {k}\")\n",
    "print(f\"  alpha (importance weight): {alpha}\")\n",
    "print(f\"  penalty: {penalty}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hetlri2cfr",
   "metadata": {},
   "source": [
    "## Step 6: Run QUBO Optimization with Different Importance Metrics\n",
    "\n",
    "This section runs QUBO optimization using each importance metric and selects the top k APs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vk6pic4bez",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Importance methods to test\n",
    "importance_methods = {\n",
    "    'mutual_info': importance_mutual_info,\n",
    "    'entropy': importance_entropy,\n",
    "    'average': importance_average,\n",
    "    'max': importance_max,\n",
    "    'variance': importance_variance\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RUNNING QUBO OPTIMIZATION FOR EACH IMPORTANCE METRIC\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run QUBO for each importance metric\n",
    "for label, imp_dict in importance_methods.items():\n",
    "    print(f\"\\n[{label.upper()}]\")\n",
    "    \n",
    "    # Check for zero importance scores\n",
    "    nonzero_scores = [v for v in imp_dict.values() if v > 0]\n",
    "    if len(nonzero_scores) == 0:\n",
    "        print(f\"  ✗ Skipped: all importance scores are zero or negative.\")\n",
    "        continue\n",
    "\n",
    "    # 1. Formulate QUBO\n",
    "    print(f\"  → Formulating QUBO (k={k}, alpha={alpha}, penalty={penalty})...\")\n",
    "    Q, relevant_aps, offset = formulate_qubo(imp_dict, redundancy_matrix, k, alpha, penalty)\n",
    "    \n",
    "    if len(relevant_aps) == 0:\n",
    "        print(f\"  ✗ Skipped: no relevant APs selected after QUBO formulation.\")\n",
    "        continue\n",
    "\n",
    "    # 2. Solve QUBO with OpenJij\n",
    "    print(f\"  → Solving QUBO with OpenJij SQA...\")\n",
    "    selected_indices, duration = solve_qubo_with_openjij(Q)\n",
    "    \n",
    "    if len(selected_indices) == 0:\n",
    "        print(f\"  ✗ Skipped: QUBO solver did not select any APs.\")\n",
    "        continue\n",
    "\n",
    "    # Map indices to AP names\n",
    "    selected_aps = [relevant_aps[i] for i in selected_indices]\n",
    "    \n",
    "    # Store preliminary results\n",
    "    results[label] = {\n",
    "        'selected_aps': selected_aps,\n",
    "        'num_aps': len(selected_aps),\n",
    "        'qubo_duration': duration,\n",
    "        'Q_matrix': Q,\n",
    "        'relevant_aps': relevant_aps\n",
    "    }\n",
    "    \n",
    "    print(f\"  ✓ Selected {len(selected_aps)} APs in {duration:.2f}s\")\n",
    "    print(f\"    APs: {', '.join(selected_aps[:5])}{'...' if len(selected_aps) > 5 else ''}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"✓ QUBO optimization completed for {len(results)} methods\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qomu95fbg8",
   "metadata": {},
   "source": [
    "## Step 7: Train ML Models on Selected AP Subsets\n",
    "\n",
    "For each importance method, train a Random Forest regressor using the selected APs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s2nchi3x8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING ML MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train models for each method\n",
    "for label in results.keys():\n",
    "    print(f\"\\n[{label.upper()}]\")\n",
    "    \n",
    "    selected_aps = results[label]['selected_aps']\n",
    "    \n",
    "    # Train Random Forest regressor\n",
    "    print(f\"  → Training Random Forest with {len(selected_aps)} APs...\")\n",
    "    models, predictions = train_regressor(\n",
    "        rssi_train, coords_train, \n",
    "        rssi_val, coords_val, \n",
    "        selected_aps\n",
    "    )\n",
    "    \n",
    "    # Get validation predictions\n",
    "    preds = predictions['rf_val']\n",
    "    \n",
    "    # Store model and predictions\n",
    "    results[label]['models'] = models\n",
    "    results[label]['predictions'] = predictions\n",
    "    results[label]['preds_val'] = preds\n",
    "    \n",
    "    print(f\"  ✓ Model trained successfully\")\n",
    "    if 'rf' in models:\n",
    "        oob_score = models['rf'].estimators_[0].oob_score_ if hasattr(models['rf'].estimators_[0], 'oob_score_') else 'N/A'\n",
    "        print(f\"    Validation predictions shape: {preds.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"✓ ML models trained for {len(results)} methods\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wstzxvr64t",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate Positioning Accuracy\n",
    "\n",
    "Calculate comprehensive metrics for each method including 3D positioning error and floor accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g4c1f8hr7hc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EVALUATING POSITIONING ACCURACY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate each method\n",
    "for label in results.keys():\n",
    "    print(f\"\\n[{label.upper()}]\")\n",
    "    \n",
    "    preds = results[label]['preds_val']\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    print(f\"  → Calculating positioning metrics...\")\n",
    "    _, _, metrics = calculate_comprehensive_metrics(\n",
    "        coords_val, preds,\n",
    "        LON_MIN, LON_MAX,\n",
    "        LAT_MIN, LAT_MAX,\n",
    "        FLOOR_HEIGHT\n",
    "    )\n",
    "    \n",
    "    # Store metrics\n",
    "    results[label]['mean_3d_error'] = metrics['real_mean_m']\n",
    "    results[label]['median_3d_error'] = metrics['real_median_m']\n",
    "    results[label]['min_error'] = metrics['real_min_m']\n",
    "    results[label]['max_error'] = metrics['real_max_m']\n",
    "    results[label]['floor_accuracy'] = metrics['floor_accuracy']\n",
    "    results[label]['all_metrics'] = metrics\n",
    "    \n",
    "    print(f\"  ✓ Metrics calculated:\")\n",
    "    print(f\"    Mean 3D Error: {metrics['real_mean_m']:.2f} m\")\n",
    "    print(f\"    Median 3D Error: {metrics['real_median_m']:.2f} m\")\n",
    "    print(f\"    Floor Accuracy: {metrics['floor_accuracy']:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"✓ Evaluation completed for {len(results)} methods\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6m8zb4sbv6p",
   "metadata": {},
   "source": [
    "## Step 9: Compare Results Across All Methods\n",
    "\n",
    "Create a comprehensive comparison table and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l3eljmqv91o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_data = []\n",
    "for label, data in results.items():\n",
    "    results_data.append({\n",
    "        'Importance_Method': label.upper(),\n",
    "        'Num_APs': data['num_aps'],\n",
    "        'Selected_APs': ', '.join(data['selected_aps']),\n",
    "        'Mean_3D_Error_m': data['mean_3d_error'],\n",
    "        'Median_3D_Error_m': data['median_3d_error'],\n",
    "        'Min_Error_m': data['min_error'],\n",
    "        'Max_Error_m': data['max_error'],\n",
    "        'Floor_Accuracy': data['floor_accuracy'],\n",
    "        'QUBO_Duration_s': data['qubo_duration']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Sort by mean 3D error (best first)\n",
    "results_df = results_df.sort_values('Mean_3D_Error_m', ascending=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RESULTS COMPARISON - ALL IMPORTANCE METHODS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nSummary Table (sorted by Mean 3D Error):\")\n",
    "print(results_df[['Importance_Method', 'Num_APs', 'Mean_3D_Error_m', \n",
    "                   'Median_3D_Error_m', 'Floor_Accuracy', 'QUBO_Duration_s']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST PERFORMING METHOD\")\n",
    "print(\"=\"*80)\n",
    "best_method = results_df.iloc[0]\n",
    "print(f\"Method: {best_method['Importance_Method']}\")\n",
    "print(f\"Mean 3D Error: {best_method['Mean_3D_Error_m']:.2f} m\")\n",
    "print(f\"Median 3D Error: {best_method['Median_3D_Error_m']:.2f} m\")\n",
    "print(f\"Floor Accuracy: {best_method['Floor_Accuracy']:.2%}\")\n",
    "print(f\"Number of APs: {best_method['Num_APs']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bg54t9hsx7w",
   "metadata": {},
   "source": [
    "## Step 10: Save Results\n",
    "\n",
    "Save the results to Excel and CSV files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4v08ayg7kzj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('../../data') / 'results'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as Excel\n",
    "excel_path = output_dir / 'pipeline_experiment_results.xlsx'\n",
    "results_df.to_excel(excel_path, index=False)\n",
    "print(f\"✓ Results saved to Excel: {excel_path}\")\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = output_dir / 'pipeline_experiment_results.csv'\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"✓ Results saved to CSV: {csv_path}\")\n",
    "\n",
    "# Also save a detailed version with selected APs\n",
    "detailed_path = output_dir / 'pipeline_experiment_detailed.xlsx'\n",
    "with pd.ExcelWriter(detailed_path, engine='openpyxl') as writer:\n",
    "    # Summary sheet\n",
    "    results_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    \n",
    "    # Individual sheets for each method's selected APs\n",
    "    for label, data in results.items():\n",
    "        ap_df = pd.DataFrame({\n",
    "            'AP_Name': data['selected_aps'],\n",
    "            'Index': range(len(data['selected_aps']))\n",
    "        })\n",
    "        sheet_name = f\"{label.upper()}_APs\"[:31]  # Excel sheet name limit\n",
    "        ap_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"✓ Detailed results saved to: {detailed_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ ALL RESULTS SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
