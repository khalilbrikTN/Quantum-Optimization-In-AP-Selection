{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# System Pipeline: Load Preprocessed Data, Importance Scores, and Redundancy\n",
    "\n",
    "This notebook demonstrates the complete pipeline for loading:\n",
    "1. Preprocessed data for Building 1\n",
    "2. Importance metrics from saved scores\n",
    "3. Redundancy data\n",
    "\n",
    "This is a streamlined version that loads pre-computed data without re-running expensive computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f247a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Python: C:\\Users\\Mohamed Khalil\\Desktop\\Quantum-Optimization-In-AP-Selection\\venv\\Scripts\\python.exe\n",
      "\n",
      "✓ Packages installed!\n",
      "Now restart your kernel: Kernel → Restart Kernel\n",
      "Then re-run all cells\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(f\"Current Python: {sys.executable}\")\n",
    "\n",
    "# Install packages in the current notebook environment\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openjij\", \"dwave-ocean-sdk\", \"-q\"])\n",
    "\n",
    "print(\"\\n✓ Packages installed!\")\n",
    "print(\"Now restart your kernel: Kernel → Restart Kernel\")\n",
    "print(\"Then re-run all cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "j0f9mfzfv1k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Added project root to Python path: C:\\Users\\Mohamed Khalil\\Desktop\\Quantum-Optimization-In-AP-Selection\n"
     ]
    }
   ],
   "source": [
    "# Add project root to Python path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the project root (2 levels up from this notebook)\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"✓ Added project root to Python path: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "# Import custom data loading functions\n",
    "from scripts.data.data_loaders import (\n",
    "    load_preprocessed_data,\n",
    "    load_all_precomputed_data,\n",
    "    load_importance_dict_from_csv,\n",
    "    load_redundancy_matrix_from_csv\n",
    ")\n",
    "\n",
    "# Import QUBO optimization functions\n",
    "from scripts.optimization.QUBO import (\n",
    "    formulate_qubo,\n",
    "    solve_qubo_with_openjij,\n",
    "    solve_qubo_with_SA\n",
    ")\n",
    "\n",
    "# Import ML training functions\n",
    "from scripts.ml.ML_post_processing import train_regressor\n",
    "\n",
    "# Import evaluation functions\n",
    "from scripts.evaluation.Analysis import calculate_comprehensive_metrics\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1_header",
   "metadata": {},
   "source": [
    "## Step 1: Load Preprocessed Data for Building 1\n",
    "\n",
    "This loads the preprocessed RSSI data, coordinates, and AP columns from saved files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "load_preprocessed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded preprocessed data from pickle: C:\\Users\\Mohamed Khalil\\Desktop\\Quantum-Optimization-In-AP-Selection\\data\\output_data\\preprocessed_data\\preprocessed_building_1.pkl\n",
      "  Training samples: 5196\n",
      "  Validation samples: 307\n",
      "  Number of APs: 520\n",
      "\n",
      "============================================================\n",
      "PREPROCESSED DATA SUMMARY\n",
      "============================================================\n",
      "Building ID: 1\n",
      "Training samples: 5196\n",
      "Validation samples: 307\n",
      "Number of APs: 520\n",
      "\n",
      "RSSI Training shape: (5196, 520)\n",
      "Coordinates Training shape: (5196, 3)\n",
      "RSSI Validation shape: (307, 520)\n",
      "Coordinates Validation shape: (307, 3)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Specify building ID\n",
    "building_id = 1\n",
    "\n",
    "# Load preprocessed data (uses pickle for fast loading)\n",
    "rssi_train, coords_train, rssi_val, coords_val, ap_columns = load_preprocessed_data(\n",
    "    building_id=building_id,\n",
    "    use_pickle=True  # True = fast (pickle), False = slower (Excel)\n",
    ")\n",
    "\n",
    "# Initialize and fit the coordinate scaler\n",
    "scaler_coords = MinMaxScaler()\n",
    "scaler_coords.fit(coords_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSED DATA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Building ID: {building_id}\")\n",
    "print(f\"Training samples: {rssi_train.shape[0]}\")\n",
    "print(f\"Validation samples: {rssi_val.shape[0]}\")\n",
    "print(f\"Number of APs: {len(ap_columns)}\")\n",
    "print(f\"\\nRSSI Training shape: {rssi_train.shape}\")\n",
    "print(f\"Coordinates Training shape: {coords_train.shape}\")\n",
    "print(f\"RSSI Validation shape: {rssi_val.shape}\")\n",
    "print(f\"Coordinates Validation shape: {coords_val.shape}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2_header",
   "metadata": {},
   "source": [
    "## Step 2: Load Importance Scores\n",
    "\n",
    "This loads all pre-computed importance metrics from saved CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "load_importance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Loading pre-computed importance scores and redundancy matrix\n",
      "============================================================\n",
      "\n",
      "Loading importance scores...\n",
      "[OK] Loaded 520 APs for entropy importance\n",
      "[OK] Loaded 520 APs for average importance\n",
      "[OK] Loaded 520 APs for median importance\n",
      "[OK] Loaded 520 APs for max importance\n",
      "[OK] Loaded 520 APs for variance importance\n",
      "[OK] Loaded 520 APs for mutual_info importance\n",
      "\n",
      "Loading redundancy matrix...\n",
      "[OK] Loaded redundancy matrix with shape: (520, 520)\n",
      "\n",
      "[OK] All data loaded successfully!\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "IMPORTANCE SCORES SUMMARY\n",
      "============================================================\n",
      "Entropy importance: 520 APs\n",
      "Average importance: 520 APs\n",
      "Median importance: 520 APs\n",
      "Max importance: 520 APs\n",
      "Variance importance: 520 APs\n",
      "Mutual Info importance: 520 APs\n",
      "============================================================\n",
      "\n",
      "Top 5 APs by Entropy Importance:\n",
      "  WAP248: 1.5819\n",
      "  WAP107: 1.4834\n",
      "  WAP108: 1.4807\n",
      "  WAP167: 1.4132\n",
      "  WAP166: 1.3899\n",
      "\n",
      "Top 5 APs by Mutual Information:\n",
      "  WAP167: 0.3408\n",
      "  WAP166: 0.3356\n",
      "  WAP179: 0.3326\n",
      "  WAP104: 0.3307\n",
      "  WAP119: 0.3261\n"
     ]
    }
   ],
   "source": [
    "# Load all importance dictionaries at once\n",
    "importance_dicts, _ = load_all_precomputed_data()\n",
    "\n",
    "# Access individual importance methods\n",
    "importance_entropy = importance_dicts['entropy']\n",
    "importance_average = importance_dicts['average']\n",
    "importance_median = importance_dicts['median']\n",
    "importance_max = importance_dicts['max']\n",
    "importance_variance = importance_dicts['variance']\n",
    "importance_mutual_info = importance_dicts['mutual_info']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPORTANCE SCORES SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Entropy importance: {len(importance_entropy)} APs\")\n",
    "print(f\"Average importance: {len(importance_average)} APs\")\n",
    "print(f\"Median importance: {len(importance_median)} APs\")\n",
    "print(f\"Max importance: {len(importance_max)} APs\")\n",
    "print(f\"Variance importance: {len(importance_variance)} APs\")\n",
    "print(f\"Mutual Info importance: {len(importance_mutual_info)} APs\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show top 5 APs for each method\n",
    "print(\"\\nTop 5 APs by Entropy Importance:\")\n",
    "top_entropy = sorted(importance_entropy.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for ap, score in top_entropy:\n",
    "    print(f\"  {ap}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 APs by Mutual Information:\")\n",
    "top_mi = sorted(importance_mutual_info.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for ap, score in top_mi:\n",
    "    print(f\"  {ap}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3_header",
   "metadata": {},
   "source": [
    "## Step 3: Load Redundancy Matrix\n",
    "\n",
    "This loads the pre-computed redundancy matrix from saved files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "load_redundancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Loading pre-computed importance scores and redundancy matrix\n",
      "============================================================\n",
      "\n",
      "Loading importance scores...\n",
      "[OK] Loaded 520 APs for entropy importance\n",
      "[OK] Loaded 520 APs for average importance\n",
      "[OK] Loaded 520 APs for median importance\n",
      "[OK] Loaded 520 APs for max importance\n",
      "[OK] Loaded 520 APs for variance importance\n",
      "[OK] Loaded 520 APs for mutual_info importance\n",
      "\n",
      "Loading redundancy matrix...\n",
      "[OK] Loaded redundancy matrix with shape: (520, 520)\n",
      "\n",
      "[OK] All data loaded successfully!\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "REDUNDANCY MATRIX SUMMARY\n",
      "============================================================\n",
      "Matrix shape: (520, 520)\n",
      "Matrix type: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "Redundancy statistics:\n",
      "  Mean redundancy: nan\n",
      "  Min redundancy: nan\n",
      "  Max redundancy: nan\n",
      "  Median redundancy: nan\n",
      "============================================================\n",
      "\n",
      "Sample of redundancy matrix (first 5x5):\n",
      "        WAP001  WAP002  WAP003  WAP004  WAP005\n",
      "WAP001     NaN     NaN     NaN     NaN     NaN\n",
      "WAP002     NaN     NaN     NaN     NaN     NaN\n",
      "WAP003     NaN     NaN     NaN     NaN     NaN\n",
      "WAP004     NaN     NaN     NaN     NaN     NaN\n",
      "WAP005     NaN     NaN     NaN     NaN     NaN\n"
     ]
    }
   ],
   "source": [
    "# Load redundancy matrix (second return value from load_all_precomputed_data)\n",
    "_, redundancy_matrix = load_all_precomputed_data()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REDUNDANCY MATRIX SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Matrix shape: {redundancy_matrix.shape}\")\n",
    "print(f\"Matrix type: {type(redundancy_matrix)}\")\n",
    "print(f\"\\nRedundancy statistics:\")\n",
    "print(f\"  Mean redundancy: {redundancy_matrix.values.mean():.4f}\")\n",
    "print(f\"  Min redundancy: {redundancy_matrix.values.min():.4f}\")\n",
    "print(f\"  Max redundancy: {redundancy_matrix.values.max():.4f}\")\n",
    "print(f\"  Median redundancy: {np.median(redundancy_matrix.values):.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show a sample of the redundancy matrix\n",
    "print(\"\\nSample of redundancy matrix (first 5x5):\")\n",
    "print(redundancy_matrix.iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verification_header",
   "metadata": {},
   "source": [
    "## Step 4: Verification and Summary\n",
    "\n",
    "Verify that all data has been loaded correctly and is ready for use in the optimization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "verification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PIPELINE DATA VERIFICATION\n",
      "============================================================\n",
      "✓ Preprocessed training data\n",
      "✓ Preprocessed validation data\n",
      "✓ Training coordinates\n",
      "✓ Validation coordinates\n",
      "✓ AP columns\n",
      "✓ Entropy importance\n",
      "✓ Average importance\n",
      "✓ Median importance\n",
      "✓ Max importance\n",
      "✓ Variance importance\n",
      "✓ Mutual info importance\n",
      "✓ Redundancy matrix\n",
      "============================================================\n",
      "\n",
      "✓ ALL DATA LOADED SUCCESSFULLY!\n",
      "\n",
      "The pipeline is ready. You can now:\n",
      "  1. Run QUBO optimization with different importance metrics\n",
      "  2. Train ML models on selected AP subsets\n",
      "  3. Evaluate positioning accuracy\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify all components are loaded\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE DATA VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checks = [\n",
    "    (\"Preprocessed training data\", rssi_train is not None and len(rssi_train) > 0),\n",
    "    (\"Preprocessed validation data\", rssi_val is not None and len(rssi_val) > 0),\n",
    "    (\"Training coordinates\", coords_train is not None and len(coords_train) > 0),\n",
    "    (\"Validation coordinates\", coords_val is not None and len(coords_val) > 0),\n",
    "    (\"AP columns\", ap_columns is not None and len(ap_columns) > 0),\n",
    "    (\"Entropy importance\", len(importance_entropy) > 0),\n",
    "    (\"Average importance\", len(importance_average) > 0),\n",
    "    (\"Median importance\", len(importance_median) > 0),\n",
    "    (\"Max importance\", len(importance_max) > 0),\n",
    "    (\"Variance importance\", len(importance_variance) > 0),\n",
    "    (\"Mutual info importance\", len(importance_mutual_info) > 0),\n",
    "    (\"Redundancy matrix\", redundancy_matrix is not None and redundancy_matrix.shape[0] > 0),\n",
    "]\n",
    "\n",
    "all_passed = True\n",
    "for check_name, result in checks:\n",
    "    status = \"✓\" if result else \"✗\"\n",
    "    print(f\"{status} {check_name}\")\n",
    "    if not result:\n",
    "        all_passed = False\n",
    "\n",
    "print(\"=\"*60)\n",
    "if all_passed:\n",
    "    print(\"\\n✓ ALL DATA LOADED SUCCESSFULLY!\")\n",
    "    print(\"\\nThe pipeline is ready. You can now:\")\n",
    "    print(\"  1. Run QUBO optimization with different importance metrics\")\n",
    "    print(\"  2. Train ML models on selected AP subsets\")\n",
    "    print(\"  3. Evaluate positioning accuracy\")\n",
    "else:\n",
    "    print(\"\\n✗ SOME DATA FAILED TO LOAD. Please check the above errors.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "## Step 5: Load System Parameters\n",
    "\n",
    "Load normalization parameters and configure QUBO settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "placeholder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ System parameters loaded from CSV:\n",
      "  LON_MIN: -7578.46197155118\n",
      "  LON_MAX: -7404.491683006287\n",
      "  LAT_MIN: 4864809.458700001\n",
      "  LAT_MAX: 4864959.505251184\n",
      "  FLOOR_HEIGHT: 3.0\n",
      "\n",
      "QUBO parameters:\n",
      "  k (num APs to select): 20\n",
      "  alpha (importance weight): 0.9\n",
      "  penalty: 2.0\n"
     ]
    }
   ],
   "source": [
    "# Load system parameters from CSV\n",
    "system_params_path = Path('../../data') / 'system_input' / 'system_parameters.csv'\n",
    "system_params_df = pd.read_csv(system_params_path)\n",
    "\n",
    "# Convert to dictionary for easy access\n",
    "system_params_dict = dict(zip(system_params_df['Parameter'], system_params_df['Value']))\n",
    "\n",
    "# Extract parameters\n",
    "LON_MIN = system_params_dict['LON_MIN']\n",
    "LON_MAX = system_params_dict['LON_MAX']\n",
    "LAT_MIN = system_params_dict['LAT_MIN']\n",
    "LAT_MAX = system_params_dict['LAT_MAX']\n",
    "FLOOR_HEIGHT = system_params_dict['FLOOR_HEIGHT']\n",
    "\n",
    "# QUBO parameters\n",
    "k = 20  # Number of APs to select\n",
    "alpha = 0.9  # Importance vs redundancy trade-off (higher = more importance weight)\n",
    "penalty = 2.0  # Penalty for violating the k constraint\n",
    "\n",
    "print(\"✓ System parameters loaded from CSV:\")\n",
    "print(f\"  LON_MIN: {LON_MIN}\")\n",
    "print(f\"  LON_MAX: {LON_MAX}\")\n",
    "print(f\"  LAT_MIN: {LAT_MIN}\")\n",
    "print(f\"  LAT_MAX: {LAT_MAX}\")\n",
    "print(f\"  FLOOR_HEIGHT: {FLOOR_HEIGHT}\")\n",
    "print(f\"\\nQUBO parameters:\")\n",
    "print(f\"  k (num APs to select): {k}\")\n",
    "print(f\"  alpha (importance weight): {alpha}\")\n",
    "print(f\"  penalty: {penalty}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hetlri2cfr",
   "metadata": {},
   "source": [
    "## Step 6: Run QUBO Optimization with Different Importance Metrics\n",
    "\n",
    "This section runs QUBO optimization using each importance metric and selects the top k APs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vk6pic4bez",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RUNNING QUBO OPTIMIZATION FOR EACH IMPORTANCE METRIC\n",
      "============================================================\n",
      "\n",
      "[MUTUAL_INFO]\n",
      "  → Formulating QUBO (k=20, alpha=0.9, penalty=2.0)...\n",
      "Formulating enhanced QUBO for k=20 APs selection...\n",
      "Done\n",
      "  → Solving QUBO with OpenJij SQA...\n",
      "\n",
      "Solving QUBO with OpenJij Simulated Quantum Annealing (SQA)...\n",
      "OpenJij completed in 31.5951 seconds\n",
      "  ✓ Selected 20 APs in 31.60s\n",
      "    APs: WAP015, WAP051, WAP091, WAP102, WAP106...\n",
      "\n",
      "[ENTROPY]\n",
      "  → Formulating QUBO (k=20, alpha=0.9, penalty=2.0)...\n",
      "Formulating enhanced QUBO for k=20 APs selection...\n",
      "Done\n",
      "  → Solving QUBO with OpenJij SQA...\n",
      "\n",
      "Solving QUBO with OpenJij Simulated Quantum Annealing (SQA)...\n",
      "OpenJij completed in 34.9735 seconds\n",
      "  ✓ Selected 20 APs in 34.97s\n",
      "    APs: WAP016, WAP037, WAP090, WAP091, WAP103...\n",
      "\n",
      "[AVERAGE]\n",
      "  → Formulating QUBO (k=20, alpha=0.9, penalty=2.0)...\n",
      "Formulating enhanced QUBO for k=20 APs selection...\n",
      "Done\n",
      "  → Solving QUBO with OpenJij SQA...\n",
      "\n",
      "Solving QUBO with OpenJij Simulated Quantum Annealing (SQA)...\n",
      "OpenJij completed in 35.7068 seconds\n",
      "  ✓ Selected 20 APs in 35.71s\n",
      "    APs: WAP059, WAP091, WAP101, WAP103, WAP105...\n",
      "\n",
      "[MAX]\n",
      "  → Formulating QUBO (k=20, alpha=0.9, penalty=2.0)...\n",
      "Formulating enhanced QUBO for k=20 APs selection...\n",
      "Done\n",
      "  → Solving QUBO with OpenJij SQA...\n",
      "\n",
      "Solving QUBO with OpenJij Simulated Quantum Annealing (SQA)...\n",
      "OpenJij completed in 35.1375 seconds\n",
      "  ✓ Selected 20 APs in 35.14s\n",
      "    APs: WAP046, WAP084, WAP090, WAP102, WAP105...\n",
      "\n",
      "[VARIANCE]\n",
      "  → Formulating QUBO (k=20, alpha=0.9, penalty=2.0)...\n",
      "Formulating enhanced QUBO for k=20 APs selection...\n",
      "Done\n",
      "  → Solving QUBO with OpenJij SQA...\n",
      "\n",
      "Solving QUBO with OpenJij Simulated Quantum Annealing (SQA)...\n",
      "OpenJij completed in 35.7798 seconds\n",
      "  ✓ Selected 20 APs in 35.78s\n",
      "    APs: WAP089, WAP090, WAP091, WAP101, WAP108...\n",
      "\n",
      "============================================================\n",
      "✓ QUBO optimization completed for 5 methods\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Importance methods to test\n",
    "importance_methods = {\n",
    "    'mutual_info': importance_mutual_info,\n",
    "    'entropy': importance_entropy,\n",
    "    'average': importance_average,\n",
    "    'max': importance_max,\n",
    "    'variance': importance_variance\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RUNNING QUBO OPTIMIZATION FOR EACH IMPORTANCE METRIC\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run QUBO for each importance metric\n",
    "for label, imp_dict in importance_methods.items():\n",
    "    print(f\"\\n[{label.upper()}]\")\n",
    "    \n",
    "    # Check for zero importance scores\n",
    "    nonzero_scores = [v for v in imp_dict.values() if v > 0]\n",
    "    if len(nonzero_scores) == 0:\n",
    "        print(f\"  ✗ Skipped: all importance scores are zero or negative.\")\n",
    "        continue\n",
    "\n",
    "    # 1. Formulate QUBO\n",
    "    print(f\"  → Formulating QUBO (k={k}, alpha={alpha}, penalty={penalty})...\")\n",
    "    Q, relevant_aps, offset = formulate_qubo(imp_dict, redundancy_matrix, k, alpha, penalty)\n",
    "    \n",
    "    if len(relevant_aps) == 0:\n",
    "        print(f\"  ✗ Skipped: no relevant APs selected after QUBO formulation.\")\n",
    "        continue\n",
    "\n",
    "    # 2. Solve QUBO with OpenJij\n",
    "    print(f\"  → Solving QUBO with OpenJij SQA...\")\n",
    "    selected_indices, duration = solve_qubo_with_openjij(Q)\n",
    "    \n",
    "    if len(selected_indices) == 0:\n",
    "        print(f\"  ✗ Skipped: QUBO solver did not select any APs.\")\n",
    "        continue\n",
    "\n",
    "    # Map indices to AP names\n",
    "    selected_aps = [relevant_aps[i] for i in selected_indices]\n",
    "    \n",
    "    # Store preliminary results\n",
    "    results[label] = {\n",
    "        'selected_aps': selected_aps,\n",
    "        'num_aps': len(selected_aps),\n",
    "        'qubo_duration': duration,\n",
    "        'Q_matrix': Q,\n",
    "        'relevant_aps': relevant_aps\n",
    "    }\n",
    "    \n",
    "    print(f\"  ✓ Selected {len(selected_aps)} APs in {duration:.2f}s\")\n",
    "    print(f\"    APs: {', '.join(selected_aps[:5])}{'...' if len(selected_aps) > 5 else ''}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"✓ QUBO optimization completed for {len(results)} methods\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qomu95fbg8",
   "metadata": {},
   "source": [
    "## Step 7: Train ML Models on Selected AP Subsets\n",
    "\n",
    "For each importance method, train a Random Forest regressor using the selected APs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "s2nchi3x8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING ML MODELS\n",
      "============================================================\n",
      "\n",
      "[MUTUAL_INFO]\n",
      "  → Training Random Forest with 20 APs...\n",
      "Training random forest regressor...\n",
      "✓ Enhanced Random Forest trained\n",
      "   Average OOB Score: 0.9054\n",
      "  ✓ Model trained successfully\n",
      "\n",
      "[ENTROPY]\n",
      "  → Training Random Forest with 20 APs...\n",
      "Training random forest regressor...\n",
      "✓ Enhanced Random Forest trained\n",
      "   Average OOB Score: 0.9403\n",
      "  ✓ Model trained successfully\n",
      "\n",
      "[AVERAGE]\n",
      "  → Training Random Forest with 20 APs...\n",
      "Training random forest regressor...\n",
      "✓ Enhanced Random Forest trained\n",
      "   Average OOB Score: 0.8933\n",
      "  ✓ Model trained successfully\n",
      "\n",
      "[MAX]\n",
      "  → Training Random Forest with 20 APs...\n",
      "Training random forest regressor...\n",
      "✓ Enhanced Random Forest trained\n",
      "   Average OOB Score: 0.8725\n",
      "  ✓ Model trained successfully\n",
      "\n",
      "[VARIANCE]\n",
      "  → Training Random Forest with 20 APs...\n",
      "Training random forest regressor...\n",
      "✓ Enhanced Random Forest trained\n",
      "   Average OOB Score: 0.8128\n",
      "  ✓ Model trained successfully\n",
      "\n",
      "============================================================\n",
      "✓ ML models trained for 5 methods\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING ML MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train models for each method\n",
    "for label in results.keys():\n",
    "    print(f\"\\n[{label.upper()}]\")\n",
    "    \n",
    "    selected_aps = results[label]['selected_aps']\n",
    "    \n",
    "    # Train Random Forest regressor\n",
    "    print(f\"  → Training Random Forest with {len(selected_aps)} APs...\")\n",
    "    models, predictions = train_regressor(\n",
    "        rssi_train, coords_train, \n",
    "        rssi_val, coords_val, \n",
    "        selected_aps\n",
    "    )\n",
    "    \n",
    "    # Get validation predictions\n",
    "    preds = predictions['rf_val']\n",
    "    \n",
    "    # Store model and predictions\n",
    "    results[label]['models'] = models\n",
    "    results[label]['predictions'] = predictions\n",
    "    results[label]['preds_val'] = preds\n",
    "    \n",
    "    print(f\"  ✓ Model trained successfully\")\n",
    "    if 'rf' in models:\n",
    "        oob_score = models['rf'].estimators_[0].oob_score_ if hasattr(models['rf'].estimators_[0], 'oob_score_') else 'N/A'\n",
    "        print(f\"    Validation predictions shape: {preds.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"✓ ML models trained for {len(results)} methods\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wstzxvr64t",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate Positioning Accuracy\n",
    "\n",
    "Calculate comprehensive metrics for each method including 3D positioning error and floor accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "g4c1f8hr7hc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATING POSITIONING ACCURACY\n",
      "============================================================\n",
      "\n",
      "[MUTUAL_INFO]\n",
      "  → Calculating positioning metrics...\n",
      "  ✓ Metrics calculated:\n",
      "    Mean 3D Error: 18.58 m\n",
      "    Median 3D Error: 13.62 m\n",
      "    Floor Accuracy (exact): 69.38%\n",
      "    Floor Accuracy (±1): 97.39%\n",
      "    Floor Accuracy (±2): 100.00%\n",
      "\n",
      "[ENTROPY]\n",
      "  → Calculating positioning metrics...\n",
      "  ✓ Metrics calculated:\n",
      "    Mean 3D Error: 17.00 m\n",
      "    Median 3D Error: 11.58 m\n",
      "    Floor Accuracy (exact): 66.12%\n",
      "    Floor Accuracy (±1): 98.37%\n",
      "    Floor Accuracy (±2): 100.00%\n",
      "\n",
      "[AVERAGE]\n",
      "  → Calculating positioning metrics...\n",
      "  ✓ Metrics calculated:\n",
      "    Mean 3D Error: 18.29 m\n",
      "    Median 3D Error: 12.36 m\n",
      "    Floor Accuracy (exact): 58.63%\n",
      "    Floor Accuracy (±1): 96.09%\n",
      "    Floor Accuracy (±2): 99.67%\n",
      "\n",
      "[MAX]\n",
      "  → Calculating positioning metrics...\n",
      "  ✓ Metrics calculated:\n",
      "    Mean 3D Error: 17.93 m\n",
      "    Median 3D Error: 13.98 m\n",
      "    Floor Accuracy (exact): 65.80%\n",
      "    Floor Accuracy (±1): 96.09%\n",
      "    Floor Accuracy (±2): 99.67%\n",
      "\n",
      "[VARIANCE]\n",
      "  → Calculating positioning metrics...\n",
      "  ✓ Metrics calculated:\n",
      "    Mean 3D Error: 17.59 m\n",
      "    Median 3D Error: 13.53 m\n",
      "    Floor Accuracy (exact): 66.45%\n",
      "    Floor Accuracy (±1): 94.46%\n",
      "    Floor Accuracy (±2): 99.67%\n",
      "\n",
      "============================================================\n",
      "✓ Evaluation completed for 5 methods\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EVALUATING POSITIONING ACCURACY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate each method\n",
    "for label in results.keys():\n",
    "    print(f\"\\n[{label.upper()}]\")\n",
    "    \n",
    "    preds = results[label]['preds_val']\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    print(f\"  → Calculating positioning metrics...\")\n",
    "    _, _, metrics = calculate_comprehensive_metrics(\n",
    "        coords_val, preds,\n",
    "        LON_MIN, LON_MAX,\n",
    "        LAT_MIN, LAT_MAX,\n",
    "        FLOOR_HEIGHT\n",
    "    )\n",
    "    \n",
    "    # Store metrics - now including all 3 floor accuracy levels\n",
    "    results[label]['mean_3d_error'] = metrics['real_mean_m']\n",
    "    results[label]['median_3d_error'] = metrics['real_median_m']\n",
    "    results[label]['min_error'] = metrics['real_min_m']\n",
    "    results[label]['max_error'] = metrics['real_max_m']\n",
    "    results[label]['floor_accuracy_0'] = metrics['floor_accuracy_0']\n",
    "    results[label]['floor_accuracy_1'] = metrics['floor_accuracy_1']\n",
    "    results[label]['floor_accuracy_2'] = metrics['floor_accuracy_2']\n",
    "    results[label]['all_metrics'] = metrics\n",
    "    \n",
    "    print(f\"  ✓ Metrics calculated:\")\n",
    "    print(f\"    Mean 3D Error: {metrics['real_mean_m']:.2f} m\")\n",
    "    print(f\"    Median 3D Error: {metrics['real_median_m']:.2f} m\")\n",
    "    print(f\"    Floor Accuracy (exact): {metrics['floor_accuracy_0']:.2%}\")\n",
    "    print(f\"    Floor Accuracy (±1): {metrics['floor_accuracy_1']:.2%}\")\n",
    "    print(f\"    Floor Accuracy (±2): {metrics['floor_accuracy_2']:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"✓ Evaluation completed for {len(results)} methods\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6m8zb4sbv6p",
   "metadata": {},
   "source": [
    "## Step 9: Compare Results Across All Methods\n",
    "\n",
    "Create a comprehensive comparison table and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "l3eljmqv91o",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "RESULTS COMPARISON - ALL IMPORTANCE METHODS\n",
      "====================================================================================================\n",
      "\n",
      "Summary Table (sorted by Mean 3D Error):\n",
      "Importance_Method  Num_APs                        Selected_APs_Display  Mean_3D_Error_m  Median_3D_Error_m  Min_Error_m  Max_Error_m  Floor_Accuracy_0  Floor_Accuracy_1  Floor_Accuracy_2\n",
      "          ENTROPY       20 WAP016, WAP037, WAP090, WAP091, WAP103, ...        16.997920          11.578227     0.206596    75.044313          0.661238          0.983713          1.000000\n",
      "         VARIANCE       20 WAP089, WAP090, WAP091, WAP101, WAP108, ...        17.587196          13.526260     0.951917    70.356851          0.664495          0.944625          0.996743\n",
      "              MAX       20 WAP046, WAP084, WAP090, WAP102, WAP105, ...        17.933808          13.978657     0.496442    72.255256          0.657980          0.960912          0.996743\n",
      "          AVERAGE       20 WAP059, WAP091, WAP101, WAP103, WAP105, ...        18.288905          12.355929     0.695355   124.481938          0.586319          0.960912          0.996743\n",
      "      MUTUAL_INFO       20 WAP015, WAP051, WAP091, WAP102, WAP106, ...        18.583199          13.622136     0.800666    83.000725          0.693811          0.973941          1.000000\n",
      "\n",
      "====================================================================================================\n",
      "BEST PERFORMING METHOD\n",
      "====================================================================================================\n",
      "Method: ENTROPY\n",
      "Mean 3D Error: 17.00 m\n",
      "Median 3D Error: 11.58 m\n",
      "Floor Accuracy (exact): 66.12%\n",
      "Floor Accuracy (±1 floor): 98.37%\n",
      "Floor Accuracy (±2 floors): 100.00%\n",
      "Number of APs: 20\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create results DataFrame\n",
    "results_data = []\n",
    "for label, data in results.items():\n",
    "    # Abbreviate Selected_APs list for display (first 5 APs)\n",
    "    selected_aps_full = data['selected_aps']\n",
    "    if len(selected_aps_full) > 5:\n",
    "        selected_aps_display = ', '.join(selected_aps_full[:5]) + ', ...'\n",
    "    else:\n",
    "        selected_aps_display = ', '.join(selected_aps_full)\n",
    "    \n",
    "    results_data.append({\n",
    "        'Importance_Method': label.upper(),\n",
    "        'Num_APs': data['num_aps'],\n",
    "        'Selected_APs': ', '.join(selected_aps_full),  # Full list for Excel\n",
    "        'Selected_APs_Display': selected_aps_display,  # Abbreviated for display\n",
    "        'Mean_3D_Error_m': data['mean_3d_error'],\n",
    "        'Median_3D_Error_m': data['median_3d_error'],\n",
    "        'Min_Error_m': data['min_error'],\n",
    "        'Max_Error_m': data['max_error'],\n",
    "        'Floor_Accuracy_0': data['floor_accuracy_0'],\n",
    "        'Floor_Accuracy_1': data['floor_accuracy_1'],\n",
    "        'Floor_Accuracy_2': data['floor_accuracy_2']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Sort by mean 3D error (best first)\n",
    "results_df = results_df.sort_values('Mean_3D_Error_m', ascending=True)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"RESULTS COMPARISON - ALL IMPORTANCE METHODS\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nSummary Table (sorted by Mean 3D Error):\")\n",
    "# Display abbreviated version\n",
    "display_cols = ['Importance_Method', 'Num_APs', 'Selected_APs_Display', \n",
    "                'Mean_3D_Error_m', 'Median_3D_Error_m', 'Min_Error_m', 'Max_Error_m',\n",
    "                'Floor_Accuracy_0', 'Floor_Accuracy_1', 'Floor_Accuracy_2']\n",
    "print(results_df[display_cols].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"BEST PERFORMING METHOD\")\n",
    "print(\"=\"*100)\n",
    "best_method = results_df.iloc[0]\n",
    "print(f\"Method: {best_method['Importance_Method']}\")\n",
    "print(f\"Mean 3D Error: {best_method['Mean_3D_Error_m']:.2f} m\")\n",
    "print(f\"Median 3D Error: {best_method['Median_3D_Error_m']:.2f} m\")\n",
    "print(f\"Floor Accuracy (exact): {best_method['Floor_Accuracy_0']:.2%}\")\n",
    "print(f\"Floor Accuracy (±1 floor): {best_method['Floor_Accuracy_1']:.2%}\")\n",
    "print(f\"Floor Accuracy (±2 floors): {best_method['Floor_Accuracy_2']:.2%}\")\n",
    "print(f\"Number of APs: {best_method['Num_APs']}\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bg54t9hsx7w",
   "metadata": {},
   "source": [
    "## Step 10: Save Results\n",
    "\n",
    "Save the results to Excel and CSV files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4v08ayg7kzj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Results saved to Excel: ..\\..\\data\\results\\pipeline_experiment_results.xlsx\n",
      "✓ Results saved to CSV: ..\\..\\data\\results\\pipeline_experiment_results.csv\n",
      "✓ Detailed results saved to: ..\\..\\data\\results\\pipeline_experiment_detailed.xlsx\n",
      "\n",
      "============================================================\n",
      "✓ ALL RESULTS SAVED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "Saved columns:\n",
      "  - Importance_Method\n",
      "  - Num_APs\n",
      "  - Selected_APs\n",
      "  - Mean_3D_Error_m\n",
      "  - Median_3D_Error_m\n",
      "  - Min_Error_m\n",
      "  - Max_Error_m\n",
      "  - Floor_Accuracy_0\n",
      "  - Floor_Accuracy_1\n",
      "  - Floor_Accuracy_2\n",
      "\n",
      "Note: QUBO_Duration_s has been removed from output as requested.\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('../../data') / 'results'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Select columns to save (exclude the display-only column)\n",
    "save_cols = ['Importance_Method', 'Num_APs', 'Selected_APs', \n",
    "             'Mean_3D_Error_m', 'Median_3D_Error_m', 'Min_Error_m', 'Max_Error_m',\n",
    "             'Floor_Accuracy_0', 'Floor_Accuracy_1', 'Floor_Accuracy_2']\n",
    "\n",
    "results_df_save = results_df[save_cols]\n",
    "\n",
    "# Save as Excel\n",
    "excel_path = output_dir / 'pipeline_experiment_results.xlsx'\n",
    "results_df_save.to_excel(excel_path, index=False)\n",
    "print(f\"✓ Results saved to Excel: {excel_path}\")\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = output_dir / 'pipeline_experiment_results.csv'\n",
    "results_df_save.to_csv(csv_path, index=False)\n",
    "print(f\"✓ Results saved to CSV: {csv_path}\")\n",
    "\n",
    "# Also save a detailed version with selected APs\n",
    "detailed_path = output_dir / 'pipeline_experiment_detailed.xlsx'\n",
    "with pd.ExcelWriter(detailed_path, engine='openpyxl') as writer:\n",
    "    # Summary sheet with all metrics\n",
    "    results_df_save.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    \n",
    "    # Individual sheets for each method's selected APs\n",
    "    for label, data in results.items():\n",
    "        ap_df = pd.DataFrame({\n",
    "            'AP_Name': data['selected_aps'],\n",
    "            'Index': range(len(data['selected_aps']))\n",
    "        })\n",
    "        sheet_name = f\"{label.upper()}_APs\"[:31]  # Excel sheet name limit\n",
    "        ap_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"✓ Detailed results saved to: {detailed_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ ALL RESULTS SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSaved columns:\")\n",
    "for col in save_cols:\n",
    "    print(f\"  - {col}\")\n",
    "print(\"\\nNote: QUBO_Duration_s has been removed from output as requested.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a1b3e-ac91-42ec-ad58-6fbf7ffc43bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6436e2-ccaf-43cf-a389-18012998640d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e75a1f-1b72-44cc-87eb-6a11e105f0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eef2409-ef27-43df-9f38-3a77f8bf199a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
