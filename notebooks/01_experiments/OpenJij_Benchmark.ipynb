{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenJij QUBO Solver Benchmark\n",
    "\n",
    "This notebook provides a comprehensive benchmark of the OpenJij Simulated Quantum Annealing (SQA) solver for the Access Point (AP) selection problem.\n",
    "\n",
    "## Benchmark Objectives\n",
    "\n",
    "1. **Time to Solution (TTS)**: Measure how long it takes to find optimal solutions\n",
    "2. **Solution Quality**: Evaluate energy levels and constraint satisfaction\n",
    "3. **Parameter Sensitivity**: Test how different parameters affect performance\n",
    "4. **Success Probability**: Calculate probability of finding optimal/near-optimal solutions\n",
    "\n",
    "## Parameters to Benchmark\n",
    "\n",
    "- **num_reads**: Number of times to run the solver (sampling count)\n",
    "- **num_sweeps**: Number of Monte Carlo steps (annealing duration)\n",
    "- **Building ID**: Test on different buildings\n",
    "- **k**: Number of APs to select\n",
    "- **alpha**: Importance vs redundancy trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import openjij as oj\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"OpenJij version: {oj.__version__}\")\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom modules\n",
    "from scripts.data.data_loaders import load_preprocessed_data, load_all_precomputed_data\n",
    "from scripts.optimization.QUBO import formulate_qubo\n",
    "from scripts.ml.ML_post_processing import train_regressor\n",
    "from scripts.evaluation.Analysis import calculate_comprehensive_metrics\n",
    "\n",
    "print(\"✓ Custom modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Parameters\n",
    "\n",
    "Configure all experiment parameters in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Building selection\n",
    "BUILDING_ID = 1  # Options: 0, 1, 2\n",
    "\n",
    "# QUBO parameters\n",
    "K = 20  # Number of APs to select\n",
    "ALPHA = 0.9  # Importance weight (0.0 to 1.0)\n",
    "PENALTY = 2.0  # Constraint penalty\n",
    "IMPORTANCE_METHOD = 'average'  # Options: 'entropy', 'average', 'max', 'variance', 'mutual_info'\n",
    "\n",
    "# Benchmark parameters\n",
    "NUM_READS_RANGE = [10, 50, 100, 200, 500, 1000]  # Different sampling counts\n",
    "NUM_SWEEPS_RANGE = [100, 500, 1000, 2000, 5000]  # Different annealing durations\n",
    "\n",
    "# Benchmark settings\n",
    "NUM_REPETITIONS = 10  # Number of times to repeat each experiment for statistical significance\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Floor height for evaluation\n",
    "FLOOR_HEIGHT = 3.0\n",
    "\n",
    "# Results output directory\n",
    "OUTPUT_DIR = Path('data') / 'results' / 'benchmarks'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BENCHMARK CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Building ID: {BUILDING_ID}\")\n",
    "print(f\"Number of APs to select (k): {K}\")\n",
    "print(f\"Alpha (importance weight): {ALPHA}\")\n",
    "print(f\"Penalty: {PENALTY}\")\n",
    "print(f\"Importance method: {IMPORTANCE_METHOD}\")\n",
    "print(f\"\\nBenchmark ranges:\")\n",
    "print(f\"  num_reads: {NUM_READS_RANGE}\")\n",
    "print(f\"  num_sweeps: {NUM_SWEEPS_RANGE}\")\n",
    "print(f\"  Repetitions per experiment: {NUM_REPETITIONS}\")\n",
    "print(f\"\\nTotal experiments: {len(NUM_READS_RANGE) * len(NUM_SWEEPS_RANGE) * NUM_REPETITIONS}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data for the selected building\n",
    "print(f\"Loading preprocessed data for Building {BUILDING_ID}...\")\n",
    "rssi_train, coords_train, rssi_val, coords_val, ap_columns = load_preprocessed_data(\n",
    "    building_id=BUILDING_ID,\n",
    "    use_pickle=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Data loaded successfully\")\n",
    "print(f\"  Training samples: {len(rssi_train):,}\")\n",
    "print(f\"  Validation samples: {len(rssi_val):,}\")\n",
    "print(f\"  Number of APs: {len(ap_columns):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load importance scores and redundancy matrix\n",
    "print(\"Loading importance scores and redundancy matrix...\")\n",
    "importance_dicts_loaded, redundancy_matrix_loaded = load_all_precomputed_data()\n",
    "\n",
    "# Select the importance method\n",
    "importance_dict = importance_dicts_loaded[IMPORTANCE_METHOD]\n",
    "\n",
    "print(f\"\\n✓ Using '{IMPORTANCE_METHOD}' importance scores\")\n",
    "print(f\"  Non-zero importance scores: {sum(1 for v in importance_dict.values() if v > 0)}/{len(importance_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load system parameters for denormalization\n",
    "system_params_path = Path('data') / 'system_input' / f'system_parameters_building_{BUILDING_ID}.csv'\n",
    "system_params_df = pd.read_csv(system_params_path)\n",
    "system_params_dict = dict(zip(system_params_df['Parameter'], system_params_df['Value']))\n",
    "\n",
    "LON_MIN = system_params_dict['LON_MIN']\n",
    "LON_MAX = system_params_dict['LON_MAX']\n",
    "LAT_MIN = system_params_dict['LAT_MIN']\n",
    "LAT_MAX = system_params_dict['LAT_MAX']\n",
    "\n",
    "print(f\"\\n✓ System parameters loaded\")\n",
    "print(f\"  Longitude range: [{LON_MIN:.2f}, {LON_MAX:.2f}]\")\n",
    "print(f\"  Latitude range: [{LAT_MIN:.2f}, {LAT_MAX:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. QUBO Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formulate QUBO problem once (same for all benchmark runs)\n",
    "print(\"Formulating QUBO problem...\")\n",
    "Q, relevant_aps, offset = formulate_qubo(\n",
    "    importance_dict,\n",
    "    redundancy_matrix_loaded,\n",
    "    K,\n",
    "    ALPHA,\n",
    "    PENALTY\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ QUBO formulated\")\n",
    "print(f\"  Number of relevant APs: {len(relevant_aps)}\")\n",
    "print(f\"  QUBO size: {len(Q)} terms\")\n",
    "print(f\"  Offset: {offset:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benchmark Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_qubo_openjij(Q, num_reads=1000, num_sweeps=1000, seed=None):\n",
    "    \"\"\"\n",
    "    Solve QUBO with OpenJij SQA sampler and return detailed results.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains solution, energy, timing, and other metrics\n",
    "    \"\"\"\n",
    "    sampler = oj.SQASampler()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = sampler.sample_qubo(\n",
    "        Q,\n",
    "        num_reads=num_reads,\n",
    "        num_sweeps=num_sweeps,\n",
    "        seed=seed\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Extract best solution\n",
    "    best_solution = response.first.sample\n",
    "    best_energy = response.first.energy\n",
    "    \n",
    "    # Count selected APs\n",
    "    num_selected = sum(best_solution.values())\n",
    "    \n",
    "    # Extract all energies and calculate statistics\n",
    "    all_energies = [record.energy for record in response.record]\n",
    "    \n",
    "    return {\n",
    "        'solution': best_solution,\n",
    "        'energy': best_energy,\n",
    "        'num_selected': num_selected,\n",
    "        'time': end_time - start_time,\n",
    "        'all_energies': all_energies,\n",
    "        'min_energy': min(all_energies),\n",
    "        'mean_energy': np.mean(all_energies),\n",
    "        'std_energy': np.std(all_energies),\n",
    "        'response': response\n",
    "    }\n",
    "\n",
    "def evaluate_solution_quality(solution, relevant_aps, rssi_train, coords_train, \n",
    "                              rssi_val, coords_val, LON_MIN, LON_MAX, LAT_MIN, LAT_MAX, FLOOR_HEIGHT):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of a solution by training a model and measuring error.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains localization metrics\n",
    "    \"\"\"\n",
    "    # Get selected APs\n",
    "    selected_indices = [i for i, val in solution.items() if val == 1]\n",
    "    selected_aps = [relevant_aps[i] for i in selected_indices]\n",
    "    \n",
    "    if len(selected_aps) == 0:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Train model\n",
    "        models, predictions = train_regressor(rssi_train, coords_train, rssi_val, coords_val, selected_aps)\n",
    "        preds = predictions['rf_val']\n",
    "        \n",
    "        # Calculate metrics\n",
    "        _, _, metrics = calculate_comprehensive_metrics(\n",
    "            coords_val, preds,\n",
    "            LON_MIN, LON_MAX,\n",
    "            LAT_MIN, LAT_MAX,\n",
    "            FLOOR_HEIGHT\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'mean_3d_error': metrics['real_mean_m'],\n",
    "            'median_3d_error': metrics['real_median_m'],\n",
    "            'floor_accuracy': metrics['floor_accuracy'],\n",
    "            'num_aps': len(selected_aps)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating solution: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"✓ Benchmark functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Benchmarks\n",
    "\n",
    "### 6.1 Benchmark: Varying num_reads (fixed num_sweeps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark varying num_reads with fixed num_sweeps\n",
    "FIXED_NUM_SWEEPS = 1000\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"BENCHMARK 1: Varying num_reads (num_sweeps={FIXED_NUM_SWEEPS})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "benchmark_reads_results = []\n",
    "\n",
    "for num_reads in tqdm(NUM_READS_RANGE, desc=\"num_reads\"):\n",
    "    for rep in range(NUM_REPETITIONS):\n",
    "        seed = RANDOM_SEED + rep\n",
    "        \n",
    "        # Solve QUBO\n",
    "        result = solve_qubo_openjij(Q, num_reads=num_reads, num_sweeps=FIXED_NUM_SWEEPS, seed=seed)\n",
    "        \n",
    "        # Record results\n",
    "        benchmark_reads_results.append({\n",
    "            'num_reads': num_reads,\n",
    "            'num_sweeps': FIXED_NUM_SWEEPS,\n",
    "            'repetition': rep,\n",
    "            'time': result['time'],\n",
    "            'energy': result['energy'],\n",
    "            'min_energy': result['min_energy'],\n",
    "            'mean_energy': result['mean_energy'],\n",
    "            'std_energy': result['std_energy'],\n",
    "            'num_selected': result['num_selected'],\n",
    "            'constraint_satisfied': (result['num_selected'] == K)\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_reads = pd.DataFrame(benchmark_reads_results)\n",
    "\n",
    "print(f\"\\n✓ Benchmark 1 complete: {len(df_reads)} experiments\")\n",
    "print(f\"\\nSummary statistics:\")\n",
    "print(df_reads.groupby('num_reads')[['time', 'energy', 'num_selected']].agg(['mean', 'std']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Benchmark: Varying num_sweeps (fixed num_reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark varying num_sweeps with fixed num_reads\n",
    "FIXED_NUM_READS = 100\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"BENCHMARK 2: Varying num_sweeps (num_reads={FIXED_NUM_READS})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "benchmark_sweeps_results = []\n",
    "\n",
    "for num_sweeps in tqdm(NUM_SWEEPS_RANGE, desc=\"num_sweeps\"):\n",
    "    for rep in range(NUM_REPETITIONS):\n",
    "        seed = RANDOM_SEED + rep\n",
    "        \n",
    "        # Solve QUBO\n",
    "        result = solve_qubo_openjij(Q, num_reads=FIXED_NUM_READS, num_sweeps=num_sweeps, seed=seed)\n",
    "        \n",
    "        # Record results\n",
    "        benchmark_sweeps_results.append({\n",
    "            'num_reads': FIXED_NUM_READS,\n",
    "            'num_sweeps': num_sweeps,\n",
    "            'repetition': rep,\n",
    "            'time': result['time'],\n",
    "            'energy': result['energy'],\n",
    "            'min_energy': result['min_energy'],\n",
    "            'mean_energy': result['mean_energy'],\n",
    "            'std_energy': result['std_energy'],\n",
    "            'num_selected': result['num_selected'],\n",
    "            'constraint_satisfied': (result['num_selected'] == K)\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_sweeps = pd.DataFrame(benchmark_sweeps_results)\n",
    "\n",
    "print(f\"\\n✓ Benchmark 2 complete: {len(df_sweeps)} experiments\")\n",
    "print(f\"\\nSummary statistics:\")\n",
    "print(df_sweeps.groupby('num_sweeps')[['time', 'energy', 'num_selected']].agg(['mean', 'std']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Benchmark: Parameter Grid (num_reads × num_sweeps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full grid benchmark (smaller ranges for efficiency)\n",
    "GRID_NUM_READS = [50, 100, 200, 500]\n",
    "GRID_NUM_SWEEPS = [500, 1000, 2000]\n",
    "GRID_REPETITIONS = 5\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BENCHMARK 3: Parameter Grid (num_reads × num_sweeps)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Grid: {len(GRID_NUM_READS)} × {len(GRID_NUM_SWEEPS)} = {len(GRID_NUM_READS) * len(GRID_NUM_SWEEPS)} combinations\")\n",
    "print(f\"Total experiments: {len(GRID_NUM_READS) * len(GRID_NUM_SWEEPS) * GRID_REPETITIONS}\")\n",
    "\n",
    "benchmark_grid_results = []\n",
    "\n",
    "total = len(GRID_NUM_READS) * len(GRID_NUM_SWEEPS) * GRID_REPETITIONS\n",
    "with tqdm(total=total, desc=\"Grid Search\") as pbar:\n",
    "    for num_reads in GRID_NUM_READS:\n",
    "        for num_sweeps in GRID_NUM_SWEEPS:\n",
    "            for rep in range(GRID_REPETITIONS):\n",
    "                seed = RANDOM_SEED + rep\n",
    "                \n",
    "                # Solve QUBO\n",
    "                result = solve_qubo_openjij(Q, num_reads=num_reads, num_sweeps=num_sweeps, seed=seed)\n",
    "                \n",
    "                # Record results\n",
    "                benchmark_grid_results.append({\n",
    "                    'num_reads': num_reads,\n",
    "                    'num_sweeps': num_sweeps,\n",
    "                    'repetition': rep,\n",
    "                    'time': result['time'],\n",
    "                    'energy': result['energy'],\n",
    "                    'min_energy': result['min_energy'],\n",
    "                    'mean_energy': result['mean_energy'],\n",
    "                    'std_energy': result['std_energy'],\n",
    "                    'num_selected': result['num_selected'],\n",
    "                    'constraint_satisfied': (result['num_selected'] == K)\n",
    "                })\n",
    "                \n",
    "                pbar.update(1)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_grid = pd.DataFrame(benchmark_grid_results)\n",
    "\n",
    "print(f\"\\n✓ Benchmark 3 complete: {len(df_grid)} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Solution Quality Evaluation (on best configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate solution quality for best configurations from grid search\n",
    "print(\"=\"*80)\n",
    "print(\"BENCHMARK 4: Solution Quality Evaluation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best configurations (lowest average energy)\n",
    "grid_summary = df_grid.groupby(['num_reads', 'num_sweeps'])['energy'].mean().reset_index()\n",
    "grid_summary = grid_summary.sort_values('energy').head(5)\n",
    "\n",
    "print(\"\\nEvaluating top 5 configurations by energy:\")\n",
    "print(grid_summary)\n",
    "\n",
    "quality_results = []\n",
    "\n",
    "for _, row in tqdm(grid_summary.iterrows(), total=len(grid_summary), desc=\"Quality eval\"):\n",
    "    num_reads = int(row['num_reads'])\n",
    "    num_sweeps = int(row['num_sweeps'])\n",
    "    \n",
    "    # Run solver\n",
    "    result = solve_qubo_openjij(Q, num_reads=num_reads, num_sweeps=num_sweeps, seed=RANDOM_SEED)\n",
    "    \n",
    "    # Evaluate solution quality\n",
    "    quality = evaluate_solution_quality(\n",
    "        result['solution'], relevant_aps,\n",
    "        rssi_train, coords_train,\n",
    "        rssi_val, coords_val,\n",
    "        LON_MIN, LON_MAX, LAT_MIN, LAT_MAX, FLOOR_HEIGHT\n",
    "    )\n",
    "    \n",
    "    if quality is not None:\n",
    "        quality_results.append({\n",
    "            'num_reads': num_reads,\n",
    "            'num_sweeps': num_sweeps,\n",
    "            'energy': result['energy'],\n",
    "            'time': result['time'],\n",
    "            'mean_3d_error': quality['mean_3d_error'],\n",
    "            'median_3d_error': quality['median_3d_error'],\n",
    "            'floor_accuracy': quality['floor_accuracy']\n",
    "        })\n",
    "\n",
    "df_quality = pd.DataFrame(quality_results)\n",
    "\n",
    "print(f\"\\n✓ Quality evaluation complete\")\n",
    "print(\"\\nQuality Results:\")\n",
    "print(df_quality.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Time to Solution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time vs num_reads\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Time vs num_reads\n",
    "df_reads_summary = df_reads.groupby('num_reads')['time'].agg(['mean', 'std']).reset_index()\n",
    "axes[0].errorbar(df_reads_summary['num_reads'], df_reads_summary['mean'], \n",
    "                 yerr=df_reads_summary['std'], marker='o', capsize=5, linewidth=2)\n",
    "axes[0].set_xlabel('Number of Reads', fontsize=12)\n",
    "axes[0].set_ylabel('Time (seconds)', fontsize=12)\n",
    "axes[0].set_title(f'Time vs num_reads (num_sweeps={FIXED_NUM_SWEEPS})', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Time vs num_sweeps\n",
    "df_sweeps_summary = df_sweeps.groupby('num_sweeps')['time'].agg(['mean', 'std']).reset_index()\n",
    "axes[1].errorbar(df_sweeps_summary['num_sweeps'], df_sweeps_summary['mean'], \n",
    "                 yerr=df_sweeps_summary['std'], marker='o', capsize=5, linewidth=2, color='orange')\n",
    "axes[1].set_xlabel('Number of Sweeps', fontsize=12)\n",
    "axes[1].set_ylabel('Time (seconds)', fontsize=12)\n",
    "axes[1].set_title(f'Time vs num_sweeps (num_reads={FIXED_NUM_READS})', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'time_to_solution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Time to solution plots saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Energy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot energy vs parameters\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Energy vs num_reads\n",
    "df_reads_energy = df_reads.groupby('num_reads')['energy'].agg(['mean', 'std', 'min']).reset_index()\n",
    "axes[0].errorbar(df_reads_energy['num_reads'], df_reads_energy['mean'], \n",
    "                 yerr=df_reads_energy['std'], marker='o', capsize=5, linewidth=2, label='Mean ± Std')\n",
    "axes[0].plot(df_reads_energy['num_reads'], df_reads_energy['min'], \n",
    "             marker='s', linestyle='--', linewidth=2, label='Min Energy')\n",
    "axes[0].set_xlabel('Number of Reads', fontsize=12)\n",
    "axes[0].set_ylabel('Energy', fontsize=12)\n",
    "axes[0].set_title(f'Energy vs num_reads (num_sweeps={FIXED_NUM_SWEEPS})', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Energy vs num_sweeps\n",
    "df_sweeps_energy = df_sweeps.groupby('num_sweeps')['energy'].agg(['mean', 'std', 'min']).reset_index()\n",
    "axes[1].errorbar(df_sweeps_energy['num_sweeps'], df_sweeps_energy['mean'], \n",
    "                 yerr=df_sweeps_energy['std'], marker='o', capsize=5, linewidth=2, \n",
    "                 color='orange', label='Mean ± Std')\n",
    "axes[1].plot(df_sweeps_energy['num_sweeps'], df_sweeps_energy['min'], \n",
    "             marker='s', linestyle='--', linewidth=2, color='red', label='Min Energy')\n",
    "axes[1].set_xlabel('Number of Sweeps', fontsize=12)\n",
    "axes[1].set_ylabel('Energy', fontsize=12)\n",
    "axes[1].set_title(f'Energy vs num_sweeps (num_reads={FIXED_NUM_READS})', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'energy_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Energy analysis plots saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Parameter Grid Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmaps for grid search results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Prepare pivot tables\n",
    "pivot_time = df_grid.pivot_table(values='time', index='num_sweeps', columns='num_reads', aggfunc='mean')\n",
    "pivot_energy = df_grid.pivot_table(values='energy', index='num_sweeps', columns='num_reads', aggfunc='mean')\n",
    "pivot_constraint = df_grid.pivot_table(values='constraint_satisfied', index='num_sweeps', columns='num_reads', aggfunc='mean')\n",
    "\n",
    "# Time heatmap\n",
    "sns.heatmap(pivot_time, annot=True, fmt='.2f', cmap='YlOrRd', ax=axes[0], cbar_kws={'label': 'Time (s)'})\n",
    "axes[0].set_title('Average Time to Solution', fontsize=14)\n",
    "axes[0].set_xlabel('num_reads', fontsize=12)\n",
    "axes[0].set_ylabel('num_sweeps', fontsize=12)\n",
    "\n",
    "# Energy heatmap\n",
    "sns.heatmap(pivot_energy, annot=True, fmt='.2f', cmap='RdYlGn_r', ax=axes[1], cbar_kws={'label': 'Energy'})\n",
    "axes[1].set_title('Average Energy', fontsize=14)\n",
    "axes[1].set_xlabel('num_reads', fontsize=12)\n",
    "axes[1].set_ylabel('num_sweeps', fontsize=12)\n",
    "\n",
    "# Constraint satisfaction heatmap\n",
    "sns.heatmap(pivot_constraint, annot=True, fmt='.2%', cmap='RdYlGn', ax=axes[2], \n",
    "            cbar_kws={'label': 'Constraint Satisfaction Rate'})\n",
    "axes[2].set_title('Constraint Satisfaction Rate', fontsize=14)\n",
    "axes[2].set_xlabel('num_reads', fontsize=12)\n",
    "axes[2].set_ylabel('num_sweeps', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'parameter_grid_heatmaps.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Parameter grid heatmaps saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Time-Energy Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time-energy trade-off\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Aggregate grid results\n",
    "grid_agg = df_grid.groupby(['num_reads', 'num_sweeps']).agg({\n",
    "    'time': 'mean',\n",
    "    'energy': 'mean',\n",
    "    'constraint_satisfied': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Create scatter plot with color for constraint satisfaction\n",
    "scatter = plt.scatter(grid_agg['time'], grid_agg['energy'], \n",
    "                     c=grid_agg['constraint_satisfied'], \n",
    "                     s=200, cmap='RdYlGn', edgecolors='black', linewidth=1.5,\n",
    "                     alpha=0.7)\n",
    "\n",
    "# Add labels for each point\n",
    "for _, row in grid_agg.iterrows():\n",
    "    plt.annotate(f\"({int(row['num_reads'])}, {int(row['num_sweeps'])})\",\n",
    "                (row['time'], row['energy']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.colorbar(scatter, label='Constraint Satisfaction Rate')\n",
    "plt.xlabel('Time (seconds)', fontsize=12)\n",
    "plt.ylabel('Energy', fontsize=12)\n",
    "plt.title('Time-Energy Trade-off\\n(num_reads, num_sweeps)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'time_energy_tradeoff.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Time-energy trade-off plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Solution Quality vs Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot solution quality metrics\n",
    "if len(df_quality) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Mean 3D error vs time\n",
    "    axes[0].scatter(df_quality['time'], df_quality['mean_3d_error'], s=200, alpha=0.6, edgecolors='black')\n",
    "    for idx, row in df_quality.iterrows():\n",
    "        axes[0].annotate(f\"({int(row['num_reads'])}, {int(row['num_sweeps'])})\",\n",
    "                        (row['time'], row['mean_3d_error']),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    axes[0].set_xlabel('Time (seconds)', fontsize=12)\n",
    "    axes[0].set_ylabel('Mean 3D Error (meters)', fontsize=12)\n",
    "    axes[0].set_title('Localization Error vs Time', fontsize=14)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Floor accuracy vs time\n",
    "    axes[1].scatter(df_quality['time'], df_quality['floor_accuracy'] * 100, \n",
    "                   s=200, alpha=0.6, edgecolors='black', color='green')\n",
    "    for idx, row in df_quality.iterrows():\n",
    "        axes[1].annotate(f\"({int(row['num_reads'])}, {int(row['num_sweeps'])})\",\n",
    "                        (row['time'], row['floor_accuracy'] * 100),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    axes[1].set_xlabel('Time (seconds)', fontsize=12)\n",
    "    axes[1].set_ylabel('Floor Accuracy (%)', fontsize=12)\n",
    "    axes[1].set_title('Floor Accuracy vs Time', fontsize=14)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'quality_vs_time.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Solution quality plots saved\")\n",
    "else:\n",
    "    print(\"No quality data available to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall statistics\n",
    "print(\"\\n1. OVERALL STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total experiments run: {len(df_reads) + len(df_sweeps) + len(df_grid)}\")\n",
    "print(f\"Total computation time: {df_reads['time'].sum() + df_sweeps['time'].sum() + df_grid['time'].sum():.2f} seconds\")\n",
    "\n",
    "# Best configurations by energy\n",
    "print(\"\\n2. BEST CONFIGURATIONS BY ENERGY\")\n",
    "print(\"-\" * 80)\n",
    "best_energy_config = grid_agg.loc[grid_agg['energy'].idxmin()]\n",
    "print(f\"Lowest energy: {best_energy_config['energy']:.4f}\")\n",
    "print(f\"  num_reads: {int(best_energy_config['num_reads'])}\")\n",
    "print(f\"  num_sweeps: {int(best_energy_config['num_sweeps'])}\")\n",
    "print(f\"  Time: {best_energy_config['time']:.2f} seconds\")\n",
    "print(f\"  Constraint satisfied: {best_energy_config['constraint_satisfied']:.1%}\")\n",
    "\n",
    "# Best configurations by time\n",
    "print(\"\\n3. FASTEST CONFIGURATIONS (with constraint satisfaction)\")\n",
    "print(\"-\" * 80)\n",
    "satisfied = grid_agg[grid_agg['constraint_satisfied'] >= 0.8]  # At least 80% satisfaction\n",
    "if len(satisfied) > 0:\n",
    "    fastest_config = satisfied.loc[satisfied['time'].idxmin()]\n",
    "    print(f\"Fastest time: {fastest_config['time']:.2f} seconds\")\n",
    "    print(f\"  num_reads: {int(fastest_config['num_reads'])}\")\n",
    "    print(f\"  num_sweeps: {int(fastest_config['num_sweeps'])}\")\n",
    "    print(f\"  Energy: {fastest_config['energy']:.4f}\")\n",
    "    print(f\"  Constraint satisfied: {fastest_config['constraint_satisfied']:.1%}\")\n",
    "else:\n",
    "    print(\"No configuration achieved 80% constraint satisfaction\")\n",
    "\n",
    "# Best configurations by quality (if available)\n",
    "if len(df_quality) > 0:\n",
    "    print(\"\\n4. BEST CONFIGURATIONS BY LOCALIZATION QUALITY\")\n",
    "    print(\"-\" * 80)\n",
    "    best_quality_config = df_quality.loc[df_quality['mean_3d_error'].idxmin()]\n",
    "    print(f\"Lowest mean 3D error: {best_quality_config['mean_3d_error']:.2f} meters\")\n",
    "    print(f\"  num_reads: {int(best_quality_config['num_reads'])}\")\n",
    "    print(f\"  num_sweeps: {int(best_quality_config['num_sweeps'])}\")\n",
    "    print(f\"  Median 3D error: {best_quality_config['median_3d_error']:.2f} meters\")\n",
    "    print(f\"  Floor accuracy: {best_quality_config['floor_accuracy']:.1%}\")\n",
    "    print(f\"  Time: {best_quality_config['time']:.2f} seconds\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n5. RECOMMENDATIONS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Based on the benchmark results:\")\n",
    "print(f\"\\n  • For BEST QUALITY: Use num_reads={int(best_energy_config['num_reads'])}, num_sweeps={int(best_energy_config['num_sweeps'])}\")\n",
    "if len(satisfied) > 0:\n",
    "    print(f\"  • For FASTEST RESULTS: Use num_reads={int(fastest_config['num_reads'])}, num_sweeps={int(fastest_config['num_sweeps'])}\")\n",
    "print(f\"\\n  • Increasing num_sweeps improves solution quality but increases computation time\")\n",
    "print(f\"  • Increasing num_reads provides more sampling but has diminishing returns\")\n",
    "print(f\"  • Balance between time and quality depends on application requirements\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all benchmark results to CSV\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save individual benchmark results\n",
    "df_reads.to_csv(OUTPUT_DIR / f'benchmark_reads_{timestamp}.csv', index=False)\n",
    "df_sweeps.to_csv(OUTPUT_DIR / f'benchmark_sweeps_{timestamp}.csv', index=False)\n",
    "df_grid.to_csv(OUTPUT_DIR / f'benchmark_grid_{timestamp}.csv', index=False)\n",
    "\n",
    "if len(df_quality) > 0:\n",
    "    df_quality.to_csv(OUTPUT_DIR / f'benchmark_quality_{timestamp}.csv', index=False)\n",
    "\n",
    "# Save summary\n",
    "summary_data = {\n",
    "    'timestamp': [timestamp],\n",
    "    'building_id': [BUILDING_ID],\n",
    "    'k': [K],\n",
    "    'alpha': [ALPHA],\n",
    "    'penalty': [PENALTY],\n",
    "    'importance_method': [IMPORTANCE_METHOD],\n",
    "    'total_experiments': [len(df_reads) + len(df_sweeps) + len(df_grid)],\n",
    "    'best_energy': [best_energy_config['energy']],\n",
    "    'best_num_reads': [int(best_energy_config['num_reads'])],\n",
    "    'best_num_sweeps': [int(best_energy_config['num_sweeps'])],\n",
    "    'best_time': [best_energy_config['time']]\n",
    "}\n",
    "\n",
    "if len(df_quality) > 0:\n",
    "    summary_data['best_mean_3d_error'] = [best_quality_config['mean_3d_error']]\n",
    "    summary_data['best_floor_accuracy'] = [best_quality_config['floor_accuracy']]\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "df_summary.to_csv(OUTPUT_DIR / f'benchmark_summary_{timestamp}.csv', index=False)\n",
    "\n",
    "print(\"✓ All benchmark results saved to:\")\n",
    "print(f\"  {OUTPUT_DIR}/\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  - benchmark_reads_{timestamp}.csv\")\n",
    "print(f\"  - benchmark_sweeps_{timestamp}.csv\")\n",
    "print(f\"  - benchmark_grid_{timestamp}.csv\")\n",
    "if len(df_quality) > 0:\n",
    "    print(f\"  - benchmark_quality_{timestamp}.csv\")\n",
    "print(f\"  - benchmark_summary_{timestamp}.csv\")\n",
    "print(f\"  - time_to_solution.png\")\n",
    "print(f\"  - energy_analysis.png\")\n",
    "print(f\"  - parameter_grid_heatmaps.png\")\n",
    "print(f\"  - time_energy_tradeoff.png\")\n",
    "if len(df_quality) > 0:\n",
    "    print(f\"  - quality_vs_time.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This benchmark provides comprehensive insights into OpenJij's SQA solver performance for the AP selection problem. The results can guide parameter selection based on your specific requirements:\n",
    "\n",
    "- **Research/Offline**: Use higher num_sweeps (2000+) for best quality\n",
    "- **Real-time/Online**: Use lower num_sweeps (500-1000) for faster results\n",
    "- **Balanced**: Use moderate settings (num_reads=100, num_sweeps=1000)\n",
    "\n",
    "The benchmark can be re-run with different buildings, k values, or importance methods by changing the configuration parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
