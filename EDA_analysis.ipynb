{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) - UJIIndoorLoc Dataset\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis on the WiFi localization dataset.\n",
    "\n",
    "**Analysis includes:**\n",
    "- Dataset overview (training and validation)\n",
    "- Building-level statistics\n",
    "- Floor-level statistics per building\n",
    "- WiFi Access Point (AP) statistics\n",
    "- Coordinate ranges and distributions\n",
    "- All results saved to Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "‚úì Training data loaded: (19937, 529)\n",
      "‚úì Validation data loaded: (1111, 529)\n"
     ]
    }
   ],
   "source": [
    "# Define data paths\n",
    "data_dir = Path('data') / 'input_data'\n",
    "train_path = data_dir / 'TrainingData.csv'\n",
    "val_path = data_dir / 'ValidationData.csv'\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_val = pd.read_csv(val_path)\n",
    "\n",
    "print(f\"‚úì Training data loaded: {df_train.shape}\")\n",
    "print(f\"‚úì Validation data loaded: {df_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns: 529\n",
      "  - WAP (WiFi Access Points): 520\n",
      "  - Location columns: 4\n",
      "  - Metadata columns: 5\n"
     ]
    }
   ],
   "source": [
    "# Identify column types\n",
    "wap_columns = [col for col in df_train.columns if col.startswith('WAP')]\n",
    "location_columns = ['LONGITUDE', 'LATITUDE', 'FLOOR', 'BUILDINGID']\n",
    "metadata_columns = ['SPACEID', 'RELATIVEPOSITION', 'USERID', 'PHONEID', 'TIMESTAMP']\n",
    "\n",
    "print(f\"Total columns: {len(df_train.columns)}\")\n",
    "print(f\"  - WAP (WiFi Access Points): {len(wap_columns)}\")\n",
    "print(f\"  - Location columns: {len(location_columns)}\")\n",
    "print(f\"  - Metadata columns: {len(metadata_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET OVERVIEW\n",
      "============================================================\n",
      "                  Metric                                      Value\n",
      "  Total Training Samples                                      19937\n",
      "Total Validation Samples                                       1111\n",
      "Total Samples (Combined)                                      21048\n",
      "      Number of WiFi APs                                        520\n",
      "     Number of Buildings                                          3\n",
      "  Number of Unique Users                                         18\n",
      "     Number of Phone IDs                                         16\n",
      "   Date Range (Training) 2013-05-30 10:15:24 to 2013-06-20 14:15:45\n",
      " Date Range (Validation) 2013-09-19 08:12:47 to 2013-10-08 15:57:16\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "overview_stats = {\n",
    "    'Metric': [\n",
    "        'Total Training Samples',\n",
    "        'Total Validation Samples',\n",
    "        'Total Samples (Combined)',\n",
    "        'Number of WiFi APs',\n",
    "        'Number of Buildings',\n",
    "        'Number of Unique Users',\n",
    "        'Number of Phone IDs',\n",
    "        'Date Range (Training)',\n",
    "        'Date Range (Validation)'\n",
    "    ],\n",
    "    'Value': [\n",
    "        len(df_train),\n",
    "        len(df_val),\n",
    "        len(df_train) + len(df_val),\n",
    "        len(wap_columns),\n",
    "        df_train['BUILDINGID'].nunique(),\n",
    "        df_train['USERID'].nunique(),\n",
    "        df_train['PHONEID'].nunique(),\n",
    "        f\"{pd.to_datetime(df_train['TIMESTAMP'], unit='s').min()} to {pd.to_datetime(df_train['TIMESTAMP'], unit='s').max()}\",\n",
    "        f\"{pd.to_datetime(df_val['TIMESTAMP'], unit='s').min()} to {pd.to_datetime(df_val['TIMESTAMP'], unit='s').max()}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_overview = pd.DataFrame(overview_stats)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(df_overview.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BUILDING-LEVEL STATISTICS\n",
      "================================================================================\n",
      " Building_ID  Training_Fingerprints  Validation_Fingerprints  Total_Fingerprints  Number_of_Floors      Floor_List      Longitude_Range           Latitude_Range Area_Approximate_m2  Unique_Spaces\n",
      "           0                   5249                      536                5785                 4    [0, 1, 2, 3] [-7695.94, -7585.39] [4864894.80, 4865017.36]            13549.84             78\n",
      "           1                   5196                      307                5503                 4    [0, 1, 2, 3] [-7578.46, -7404.49] [4864809.46, 4864959.51]            26103.64             86\n",
      "           2                   9492                      268                9760                 5 [0, 1, 2, 3, 4] [-7415.16, -7299.79] [4864745.75, 4864861.76]            13384.95             97\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze each building\n",
    "building_stats = []\n",
    "\n",
    "for building_id in sorted(df_train['BUILDINGID'].unique()):\n",
    "    # Training data\n",
    "    train_building = df_train[df_train['BUILDINGID'] == building_id]\n",
    "    # Validation data\n",
    "    val_building = df_val[df_val['BUILDINGID'] == building_id]\n",
    "    \n",
    "    # Get floor information\n",
    "    floors_train = sorted(train_building['FLOOR'].unique())\n",
    "    floors_val = sorted(val_building['FLOOR'].unique())\n",
    "    all_floors = sorted(set(floors_train) | set(floors_val))\n",
    "    \n",
    "    # Coordinate ranges\n",
    "    lon_min = min(train_building['LONGITUDE'].min(), val_building['LONGITUDE'].min())\n",
    "    lon_max = max(train_building['LONGITUDE'].max(), val_building['LONGITUDE'].max())\n",
    "    lat_min = min(train_building['LATITUDE'].min(), val_building['LATITUDE'].min())\n",
    "    lat_max = max(train_building['LATITUDE'].max(), val_building['LATITUDE'].max())\n",
    "    \n",
    "    building_stats.append({\n",
    "        'Building_ID': building_id,\n",
    "        'Training_Fingerprints': len(train_building),\n",
    "        'Validation_Fingerprints': len(val_building),\n",
    "        'Total_Fingerprints': len(train_building) + len(val_building),\n",
    "        'Number_of_Floors': len(all_floors),\n",
    "        'Floor_List': str(all_floors),\n",
    "        'Longitude_Range': f\"[{lon_min:.2f}, {lon_max:.2f}]\",\n",
    "        'Latitude_Range': f\"[{lat_min:.2f}, {lat_max:.2f}]\",\n",
    "        'Area_Approximate_m2': f\"{abs(lon_max - lon_min) * abs(lat_max - lat_min):.2f}\",\n",
    "        'Unique_Spaces': train_building['SPACEID'].nunique()\n",
    "    })\n",
    "\n",
    "df_buildings = pd.DataFrame(building_stats)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUILDING-LEVEL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(df_buildings.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Floor-Level Analysis (Per Building)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "FLOOR-LEVEL STATISTICS (PER BUILDING)\n",
      "==========================================================================================\n",
      " Building_ID  Floor  Training_Fingerprints  Validation_Fingerprints  Total_Fingerprints AP_Coverage_%  Unique_Spaces  Unique_Users\n",
      "           0      0                   1059                       78                1137          2.57             54             2\n",
      "           0      1                   1356                      208                1564          3.15             66             2\n",
      "           0      2                   1443                      165                1608          3.42             68             2\n",
      "           0      3                   1391                       85                1476          2.92             68             2\n",
      "           1      0                   1368                       30                1398          3.29             49             5\n",
      "           1      1                   1484                      143                1627          3.51             38             5\n",
      "           1      2                   1396                       87                1483          3.59             45             4\n",
      "           1      3                    948                       47                 995          1.96             30             5\n",
      "           2      0                   1942                       24                1966          3.16             44             4\n",
      "           2      1                   2162                      111                2273          4.15             60             5\n",
      "           2      2                   1577                       54                1631          4.25             57             3\n",
      "           2      3                   2709                       40                2749          3.78             91             5\n",
      "           2      4                   1102                       39                1141          3.97             65             3\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Detailed floor analysis for each building\n",
    "floor_stats = []\n",
    "\n",
    "for building_id in sorted(df_train['BUILDINGID'].unique()):\n",
    "    train_building = df_train[df_train['BUILDINGID'] == building_id]\n",
    "    val_building = df_val[df_val['BUILDINGID'] == building_id]\n",
    "    \n",
    "    # Get all floors in this building\n",
    "    all_floors = sorted(set(train_building['FLOOR'].unique()) | set(val_building['FLOOR'].unique()))\n",
    "    \n",
    "    for floor in all_floors:\n",
    "        train_floor = train_building[train_building['FLOOR'] == floor]\n",
    "        val_floor = val_building[val_building['FLOOR'] == floor]\n",
    "        \n",
    "        # Calculate coverage (non-100 RSSI values)\n",
    "        train_waps = train_floor[wap_columns]\n",
    "        active_aps_train = (train_waps != 100).sum().sum()\n",
    "        total_possible_train = len(train_floor) * len(wap_columns)\n",
    "        coverage_train = (active_aps_train / total_possible_train * 100) if total_possible_train > 0 else 0\n",
    "        \n",
    "        floor_stats.append({\n",
    "            'Building_ID': building_id,\n",
    "            'Floor': floor,\n",
    "            'Training_Fingerprints': len(train_floor),\n",
    "            'Validation_Fingerprints': len(val_floor),\n",
    "            'Total_Fingerprints': len(train_floor) + len(val_floor),\n",
    "            'AP_Coverage_%': f\"{coverage_train:.2f}\",\n",
    "            'Unique_Spaces': train_floor['SPACEID'].nunique(),\n",
    "            'Unique_Users': train_floor['USERID'].nunique()\n",
    "        })\n",
    "\n",
    "df_floors = pd.DataFrame(floor_stats)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"FLOOR-LEVEL STATISTICS (PER BUILDING)\")\n",
    "print(\"=\"*90)\n",
    "print(df_floors.to_string(index=False))\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. WiFi Access Point (AP) Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "WIFI ACCESS POINT STATISTICS (Sample - First 10 APs)\n",
      "==========================================================================================\n",
      "AP_Name  Train_Detections Train_Detection_%  Val_Detections Val_Detection_% Train_Avg_RSSI Val_Avg_RSSI\n",
      " WAP001                18              0.09               8            0.72         -95.33       -90.62\n",
      " WAP002                19              0.10               0            0.00         -87.89          N/A\n",
      " WAP003                 0              0.00               2            0.18            N/A       -86.00\n",
      " WAP004                 0              0.00               2            0.18            N/A       -86.00\n",
      " WAP005                40              0.20               0            0.00         -92.53          N/A\n",
      " WAP006               308              1.54               0            0.00         -85.75          N/A\n",
      " WAP007               578              2.90               0            0.00         -81.65          N/A\n",
      " WAP008               677              3.40              34            3.06         -81.99       -86.35\n",
      " WAP009               595              2.98              58            5.22         -77.79       -79.00\n",
      " WAP010                87              0.44               7            0.63         -91.63       -88.57\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze AP detection rates\n",
    "ap_stats = []\n",
    "\n",
    "for wap in wap_columns[:10]:  # Show first 10 as example\n",
    "    train_detected = (df_train[wap] != 100).sum()\n",
    "    val_detected = (df_val[wap] != 100).sum()\n",
    "    \n",
    "    train_avg_rssi = df_train[df_train[wap] != 100][wap].mean()\n",
    "    val_avg_rssi = df_val[df_val[wap] != 100][wap].mean()\n",
    "    \n",
    "    ap_stats.append({\n",
    "        'AP_Name': wap,\n",
    "        'Train_Detections': train_detected,\n",
    "        'Train_Detection_%': f\"{train_detected / len(df_train) * 100:.2f}\",\n",
    "        'Val_Detections': val_detected,\n",
    "        'Val_Detection_%': f\"{val_detected / len(df_val) * 100:.2f}\",\n",
    "        'Train_Avg_RSSI': f\"{train_avg_rssi:.2f}\" if not np.isnan(train_avg_rssi) else 'N/A',\n",
    "        'Val_Avg_RSSI': f\"{val_avg_rssi:.2f}\" if not np.isnan(val_avg_rssi) else 'N/A'\n",
    "    })\n",
    "\n",
    "df_aps_sample = pd.DataFrame(ap_stats)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"WIFI ACCESS POINT STATISTICS (Sample - First 10 APs)\")\n",
    "print(\"=\"*90)\n",
    "print(df_aps_sample.to_string(index=False))\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OVERALL AP SUMMARY\n",
      "============================================================\n",
      "                                    Metric Value\n",
      "                      Total APs in Dataset   520\n",
      "                     Active APs (Training)   465\n",
      "                   Active APs (Validation)   367\n",
      "                   Inactive APs (Training)    55\n",
      "                 Inactive APs (Validation)   153\n",
      "        Avg APs per Fingerprint (Training) 17.99\n",
      "      Avg APs per Fingerprint (Validation) 16.48\n",
      "  Max APs in Single Fingerprint (Training)    51\n",
      "Max APs in Single Fingerprint (Validation)    35\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Overall AP statistics\n",
    "train_ap_matrix = df_train[wap_columns]\n",
    "val_ap_matrix = df_val[wap_columns]\n",
    "\n",
    "# Count APs that are detected at least once\n",
    "train_active_aps = ((train_ap_matrix != 100).sum() > 0).sum()\n",
    "val_active_aps = ((val_ap_matrix != 100).sum() > 0).sum()\n",
    "\n",
    "# Average number of APs detected per fingerprint\n",
    "train_avg_aps_per_fp = (train_ap_matrix != 100).sum(axis=1).mean()\n",
    "val_avg_aps_per_fp = (val_ap_matrix != 100).sum(axis=1).mean()\n",
    "\n",
    "ap_summary = {\n",
    "    'Metric': [\n",
    "        'Total APs in Dataset',\n",
    "        'Active APs (Training)',\n",
    "        'Active APs (Validation)',\n",
    "        'Inactive APs (Training)',\n",
    "        'Inactive APs (Validation)',\n",
    "        'Avg APs per Fingerprint (Training)',\n",
    "        'Avg APs per Fingerprint (Validation)',\n",
    "        'Max APs in Single Fingerprint (Training)',\n",
    "        'Max APs in Single Fingerprint (Validation)'\n",
    "    ],\n",
    "    'Value': [\n",
    "        len(wap_columns),\n",
    "        train_active_aps,\n",
    "        val_active_aps,\n",
    "        len(wap_columns) - train_active_aps,\n",
    "        len(wap_columns) - val_active_aps,\n",
    "        f\"{train_avg_aps_per_fp:.2f}\",\n",
    "        f\"{val_avg_aps_per_fp:.2f}\",\n",
    "        (train_ap_matrix != 100).sum(axis=1).max(),\n",
    "        (val_ap_matrix != 100).sum(axis=1).max()\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_ap_summary = pd.DataFrame(ap_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OVERALL AP SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(df_ap_summary.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Coordinate Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "COORDINATE DISTRIBUTION (PER BUILDING)\n",
      "========================================================================================================================\n",
      " Building_ID Longitude_Min Longitude_Max Longitude_Mean Longitude_Std   Latitude_Min   Latitude_Max  Latitude_Mean Latitude_Std\n",
      "           0  -7695.938755  -7585.389976   -7639.059782     25.072780 4864894.795848 4865017.364684 4864957.884267    32.624848\n",
      "           1  -7578.461972  -7404.491683   -7489.006959     49.442550 4864809.458700 4864959.505251 4864884.085808    36.209723\n",
      "           2  -7415.163004  -7299.786517   -7354.123134     29.865606 4864745.745016 4864861.756103 4864815.416445    28.129531\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Coordinate statistics for each building\n",
    "coord_stats = []\n",
    "\n",
    "for building_id in sorted(df_train['BUILDINGID'].unique()):\n",
    "    train_building = df_train[df_train['BUILDINGID'] == building_id]\n",
    "    val_building = df_val[df_val['BUILDINGID'] == building_id]\n",
    "    \n",
    "    all_data = pd.concat([train_building, val_building])\n",
    "    \n",
    "    coord_stats.append({\n",
    "        'Building_ID': building_id,\n",
    "        'Longitude_Min': f\"{all_data['LONGITUDE'].min():.6f}\",\n",
    "        'Longitude_Max': f\"{all_data['LONGITUDE'].max():.6f}\",\n",
    "        'Longitude_Mean': f\"{all_data['LONGITUDE'].mean():.6f}\",\n",
    "        'Longitude_Std': f\"{all_data['LONGITUDE'].std():.6f}\",\n",
    "        'Latitude_Min': f\"{all_data['LATITUDE'].min():.6f}\",\n",
    "        'Latitude_Max': f\"{all_data['LATITUDE'].max():.6f}\",\n",
    "        'Latitude_Mean': f\"{all_data['LATITUDE'].mean():.6f}\",\n",
    "        'Latitude_Std': f\"{all_data['LATITUDE'].std():.6f}\"\n",
    "    })\n",
    "\n",
    "df_coords = pd.DataFrame(coord_stats)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"COORDINATE DISTRIBUTION (PER BUILDING)\")\n",
    "print(\"=\"*120)\n",
    "print(df_coords.to_string(index=False))\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. User and Device Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "USER STATISTICS\n",
      "================================================================================\n",
      " User_ID  Total_Fingerprints  Buildings_Visited  Floors_Visited  Phone_IDs_Used  Unique_Locations\n",
      "       1                2737                  1               4               1               251\n",
      "       2                1091                  2               2               1               108\n",
      "       3                 192                  1               1               1                20\n",
      "       4                 374                  1               1               1                38\n",
      "       5                 610                  1               2               1                59\n",
      "       6                 980                  1               2               1                98\n",
      "       7                1383                  2               2               1               119\n",
      "       8                 507                  2               2               1                48\n",
      "       9                1066                  2               3               1                73\n",
      "      10                 913                  2               3               1                85\n",
      "      11                4516                  3               4               1               395\n",
      "      12                 437                  1               1               1                42\n",
      "      13                 841                  2               3               1                71\n",
      "      14                1596                  2               4               1               129\n",
      "      15                 498                  1               1               1                58\n",
      "      16                1032                  2               2               1                97\n",
      "      17                 724                  2               2               1                65\n",
      "      18                 440                  2               2               1                45\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# User statistics\n",
    "user_stats = []\n",
    "\n",
    "for user_id in sorted(df_train['USERID'].unique()):\n",
    "    user_data = df_train[df_train['USERID'] == user_id]\n",
    "    \n",
    "    user_stats.append({\n",
    "        'User_ID': user_id,\n",
    "        'Total_Fingerprints': len(user_data),\n",
    "        'Buildings_Visited': user_data['BUILDINGID'].nunique(),\n",
    "        'Floors_Visited': user_data['FLOOR'].nunique(),\n",
    "        'Phone_IDs_Used': user_data['PHONEID'].nunique(),\n",
    "        'Unique_Locations': user_data[['LONGITUDE', 'LATITUDE', 'FLOOR']].drop_duplicates().shape[0]\n",
    "    })\n",
    "\n",
    "df_users = pd.DataFrame(user_stats)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"USER STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(df_users.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PHONE DEVICE STATISTICS\n",
      "======================================================================\n",
      " Phone_ID  Total_Fingerprints  Users  Buildings  Floors\n",
      "        1                 507      1          2       2\n",
      "        3                 610      1          1       2\n",
      "        6                1383      1          2       2\n",
      "        7                1596      1          2       4\n",
      "        8                 913      1          2       3\n",
      "       10                 440      1          2       2\n",
      "       11                 498      1          1       1\n",
      "       13                4516      1          3       4\n",
      "       14                4835      3          3       4\n",
      "       16                 192      1          1       1\n",
      "       17                 841      1          2       3\n",
      "       18                 374      1          1       1\n",
      "       19                 980      1          1       2\n",
      "       22                 724      1          2       2\n",
      "       23                1091      1          2       2\n",
      "       24                 437      1          1       1\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Phone device statistics\n",
    "phone_stats = []\n",
    "\n",
    "for phone_id in sorted(df_train['PHONEID'].unique()):\n",
    "    phone_data = df_train[df_train['PHONEID'] == phone_id]\n",
    "    \n",
    "    phone_stats.append({\n",
    "        'Phone_ID': phone_id,\n",
    "        'Total_Fingerprints': len(phone_data),\n",
    "        'Users': phone_data['USERID'].nunique(),\n",
    "        'Buildings': phone_data['BUILDINGID'].nunique(),\n",
    "        'Floors': phone_data['FLOOR'].nunique()\n",
    "    })\n",
    "\n",
    "df_phones = pd.DataFrame(phone_stats)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHONE DEVICE STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(df_phones.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save All Results to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving all results to: data\\EDA\\EDA_Analysis_20251020_235415.xlsx\n",
      "================================================================================\n",
      "‚úì Sheet 1: Overview\n",
      "‚úì Sheet 2: Building Statistics\n",
      "‚úì Sheet 3: Floor Statistics\n",
      "‚úì Sheet 4: AP Summary\n",
      "‚úì Sheet 5: Top 50 Active APs\n",
      "‚úì Sheet 6: Coordinate Distribution\n",
      "‚úì Sheet 7: User Statistics\n",
      "‚úì Sheet 8: Phone Statistics\n",
      "‚úì Sheet 9: Data Quality Summary\n",
      "================================================================================\n",
      "\n",
      "‚úì‚úì‚úì All results saved successfully to:\n",
      "    data\\EDA\\EDA_Analysis_20251020_235415.xlsx\n",
      "\n",
      "The Excel file contains 9 sheets with comprehensive EDA results.\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('data') / 'EDA'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = output_dir / f'EDA_Analysis_{timestamp}.xlsx'\n",
    "\n",
    "print(f\"\\nSaving all results to: {output_file}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save all dataframes to different sheets\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    # Sheet 1: Overview\n",
    "    df_overview.to_excel(writer, sheet_name='1_Overview', index=False)\n",
    "    print(\"‚úì Sheet 1: Overview\")\n",
    "    \n",
    "    # Sheet 2: Building Statistics\n",
    "    df_buildings.to_excel(writer, sheet_name='2_Building_Stats', index=False)\n",
    "    print(\"‚úì Sheet 2: Building Statistics\")\n",
    "    \n",
    "    # Sheet 3: Floor Statistics\n",
    "    df_floors.to_excel(writer, sheet_name='3_Floor_Stats', index=False)\n",
    "    print(\"‚úì Sheet 3: Floor Statistics\")\n",
    "    \n",
    "    # Sheet 4: AP Summary\n",
    "    df_ap_summary.to_excel(writer, sheet_name='4_AP_Summary', index=False)\n",
    "    print(\"‚úì Sheet 4: AP Summary\")\n",
    "    \n",
    "    # Sheet 5: AP Sample (first 50 APs)\n",
    "    # Create full AP stats for top 50 most active APs\n",
    "    ap_full_stats = []\n",
    "    for wap in wap_columns:\n",
    "        train_detected = (df_train[wap] != 100).sum()\n",
    "        val_detected = (df_val[wap] != 100).sum()\n",
    "        train_avg_rssi = df_train[df_train[wap] != 100][wap].mean()\n",
    "        \n",
    "        ap_full_stats.append({\n",
    "            'AP_Name': wap,\n",
    "            'Train_Detections': train_detected,\n",
    "            'Train_Detection_%': train_detected / len(df_train) * 100,\n",
    "            'Val_Detections': val_detected,\n",
    "            'Train_Avg_RSSI': train_avg_rssi if not np.isnan(train_avg_rssi) else 0\n",
    "        })\n",
    "    \n",
    "    df_ap_full = pd.DataFrame(ap_full_stats)\n",
    "    df_ap_full = df_ap_full.sort_values('Train_Detections', ascending=False).head(50)\n",
    "    df_ap_full.to_excel(writer, sheet_name='5_Top50_APs', index=False)\n",
    "    print(\"‚úì Sheet 5: Top 50 Active APs\")\n",
    "    \n",
    "    # Sheet 6: Coordinate Distribution\n",
    "    df_coords.to_excel(writer, sheet_name='6_Coordinates', index=False)\n",
    "    print(\"‚úì Sheet 6: Coordinate Distribution\")\n",
    "    \n",
    "    # Sheet 7: User Statistics\n",
    "    df_users.to_excel(writer, sheet_name='7_User_Stats', index=False)\n",
    "    print(\"‚úì Sheet 7: User Statistics\")\n",
    "    \n",
    "    # Sheet 8: Phone Statistics\n",
    "    df_phones.to_excel(writer, sheet_name='8_Phone_Stats', index=False)\n",
    "    print(\"‚úì Sheet 8: Phone Statistics\")\n",
    "    \n",
    "    # Sheet 9: Data Quality Summary\n",
    "    quality_stats = {\n",
    "        'Metric': [\n",
    "            'Training Samples with Missing Coordinates',\n",
    "            'Validation Samples with Missing Coordinates',\n",
    "            'Training Samples with No AP Detection',\n",
    "            'Validation Samples with No AP Detection',\n",
    "            'Duplicate Fingerprints (Training)',\n",
    "            'Duplicate Fingerprints (Validation)',\n",
    "            'Overall Data Completeness (Training) %',\n",
    "            'Overall Data Completeness (Validation) %'\n",
    "        ],\n",
    "        'Value': [\n",
    "            df_train[['LONGITUDE', 'LATITUDE']].isnull().any(axis=1).sum(),\n",
    "            df_val[['LONGITUDE', 'LATITUDE']].isnull().any(axis=1).sum(),\n",
    "            ((train_ap_matrix == 100).all(axis=1)).sum(),\n",
    "            ((val_ap_matrix == 100).all(axis=1)).sum(),\n",
    "            df_train.duplicated().sum(),\n",
    "            df_val.duplicated().sum(),\n",
    "            f\"{(1 - df_train.isnull().sum().sum() / (df_train.shape[0] * df_train.shape[1])) * 100:.2f}\",\n",
    "            f\"{(1 - df_val.isnull().sum().sum() / (df_val.shape[0] * df_val.shape[1])) * 100:.2f}\"\n",
    "        ]\n",
    "    }\n",
    "    df_quality = pd.DataFrame(quality_stats)\n",
    "    df_quality.to_excel(writer, sheet_name='9_Data_Quality', index=False)\n",
    "    print(\"‚úì Sheet 9: Data Quality Summary\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úì‚úì‚úì All results saved successfully to:\\n    {output_file}\")\n",
    "print(\"\\nThe Excel file contains 9 sheets with comprehensive EDA results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPLORATORY DATA ANALYSIS - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total Samples: {len(df_train) + len(df_val):,}\")\n",
    "print(f\"   ‚Ä¢ Training: {len(df_train):,} | Validation: {len(df_val):,}\")\n",
    "print(f\"   ‚Ä¢ WiFi Access Points: {len(wap_columns)}\")\n",
    "print(f\"   ‚Ä¢ Buildings: {df_train['BUILDINGID'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total Floors: {len(set(df_train['FLOOR'].unique()) | set(df_val['FLOOR'].unique()))}\")\n",
    "\n",
    "print(f\"\\nüè¢ Per Building:\")\n",
    "for _, row in df_buildings.iterrows():\n",
    "    print(f\"   ‚Ä¢ Building {row['Building_ID']}: {row['Total_Fingerprints']} fingerprints across {row['Number_of_Floors']} floors\")\n",
    "\n",
    "print(f\"\\nüì° WiFi Coverage:\")\n",
    "print(f\"   ‚Ä¢ Active APs (Training): {train_active_aps} / {len(wap_columns)}\")\n",
    "print(f\"   ‚Ä¢ Avg APs per Fingerprint: {train_avg_aps_per_fp:.1f}\")\n",
    "\n",
    "print(f\"\\nüë• Data Collection:\")\n",
    "print(f\"   ‚Ä¢ Unique Users: {df_train['USERID'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Phone Devices: {df_train['PHONEID'].nunique()}\")\n",
    "\n",
    "print(f\"\\nüìÅ Output:\")\n",
    "print(f\"   ‚Ä¢ Excel file saved with 9 comprehensive sheets\")\n",
    "print(f\"   ‚Ä¢ Location: {output_file}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations (Optional)\n",
    "\n",
    "Generate quick visualizations for better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Fingerprints per Building\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training vs Validation per building\n",
    "buildings = df_buildings['Building_ID']\n",
    "x = np.arange(len(buildings))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, df_buildings['Training_Fingerprints'], width, label='Training', alpha=0.8)\n",
    "ax1.bar(x + width/2, df_buildings['Validation_Fingerprints'], width, label='Validation', alpha=0.8)\n",
    "ax1.set_xlabel('Building ID', fontweight='bold')\n",
    "ax1.set_ylabel('Number of Fingerprints', fontweight='bold')\n",
    "ax1.set_title('Fingerprints per Building', fontweight='bold', fontsize=14)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(buildings)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Floors per building\n",
    "ax2.bar(buildings, df_buildings['Number_of_Floors'], color='coral', alpha=0.8)\n",
    "ax2.set_xlabel('Building ID', fontweight='bold')\n",
    "ax2.set_ylabel('Number of Floors', fontweight='bold')\n",
    "ax2.set_title('Floors per Building', fontweight='bold', fontsize=14)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'building_overview.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Saved visualization: {output_dir / 'building_overview.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Fingerprints per Floor (for each building)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, building_id in enumerate(sorted(df_train['BUILDINGID'].unique())):\n",
    "    building_floors = df_floors[df_floors['Building_ID'] == building_id]\n",
    "    \n",
    "    floors = building_floors['Floor']\n",
    "    train_counts = building_floors['Training_Fingerprints']\n",
    "    val_counts = building_floors['Validation_Fingerprints']\n",
    "    \n",
    "    x = np.arange(len(floors))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[i].bar(x - width/2, train_counts, width, label='Training', alpha=0.8, color='steelblue')\n",
    "    axes[i].bar(x + width/2, val_counts, width, label='Validation', alpha=0.8, color='orange')\n",
    "    axes[i].set_xlabel('Floor', fontweight='bold')\n",
    "    axes[i].set_ylabel('Number of Fingerprints', fontweight='bold')\n",
    "    axes[i].set_title(f'Building {building_id}', fontweight='bold', fontsize=14)\n",
    "    axes[i].set_xticks(x)\n",
    "    axes[i].set_xticklabels(floors)\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'floor_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Saved visualization: {output_dir / 'floor_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Complete!\n",
    "\n",
    "All EDA results have been saved to:\n",
    "- Excel file with 9 comprehensive sheets\n",
    "- Visualization plots (PNG files)\n",
    "\n",
    "Location: `data/EDA/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
